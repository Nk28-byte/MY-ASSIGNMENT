{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nk28-byte/MY-ASSIGNMENT/blob/main/GenAi_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theoritical Questions"
      ],
      "metadata": {
        "id": "FHHzuT_bZIfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.1 What is Generative AI?\n",
        "\n",
        "Ans :- Generative AI refers to a type of artificial intelligence that focuses on creating new content, such as text, images, music, code, or even videos, by learning patterns from existing data. Unlike traditional AI, which is designed primarily for classification, prediction, or analysis, generative AI aims to produce original and creative outputs.\n",
        "\n",
        "How It Works\n",
        "Generative AI uses machine learning models, particularly:\n",
        "\n",
        "Generative Adversarial Networks (GANs): Used to generate realistic images, videos, or other types of media by having two neural networks (a generator and a discriminator) compete against each other.\n",
        "Variational Autoencoders (VAEs): Used for generating data like images by learning to encode and decode the input.\n",
        "Transformer Models (e.g., GPT, DALL¬∑E, ChatGPT): These are specialized in processing and generating text or images by understanding contextual patterns in the data.\n",
        "\n",
        "\n",
        "Q.2 How is Generative AI different from traditional AI?\n",
        "\n",
        "Ans :- Generative AI and traditional AI differ fundamentally in their objectives and methodologies:\n",
        "\n",
        "Traditional AI:\n",
        "\n",
        "Purpose: Designed to perform specific tasks such as classification, prediction, or decision-making based on predefined rules or learned patterns.\n",
        "\n",
        "Functionality: Excels at analyzing existing data to identify patterns, make decisions, or automate processes.\n",
        "\n",
        "Examples: Spam filters that classify emails, recommendation systems suggesting products, and diagnostic tools identifying diseases from medical images.\n",
        "\n",
        "Generative AI:\n",
        "\n",
        "Purpose: Aims to create new content by learning from existing data, producing outputs that resemble human-created content.\n",
        "\n",
        "Functionality: Generates novel data such as text, images, music, or code by understanding and replicating the patterns and structures found in its training data.\n",
        "\n",
        "Examples: Models like GPT-4 generating human-like text, DALL¬∑E creating images from textual descriptions, and music composition AI producing original melodies.\n",
        "\n",
        "\n",
        "Q.3 Name two applications of Generative AI in the industry?\n",
        "\n",
        "Ans :- Generative AI is transforming various industries by enabling innovative applications that enhance efficiency and creativity. Here are two notable applications:\n",
        "\n",
        "Manufacturing Design Optimization:\n",
        "\n",
        "Application: Generative AI assists in designing optimized parts and products by analyzing specific goals and constraints, leading to innovative solutions that might not emerge through traditional design methods.\n",
        "Benefits: This approach accelerates the design process, reduces material usage, and improves product performance, thereby enhancing overall manufacturing efficiency.\n",
        "\n",
        "\n",
        "Q.4 What are some challenges associated with Generative AI?\n",
        "\n",
        "Ans :- Generative AI has made significant strides in recent years, offering innovative solutions across various sectors. However, its implementation and utilization come with several challenges:\n",
        "\n",
        "Data Quality and Bias:\n",
        "\n",
        "Issue: Generative AI models are heavily dependent on the data they are trained on. If this data contains biases or inaccuracies, the models can perpetuate or even amplify these issues, leading to outputs that are skewed or unfair.\n",
        "LEXISNEXIS\n",
        "Example: A language model trained on biased text may produce outputs that reinforce stereotypes.\n",
        "Ethical and Societal Implications:\n",
        "\n",
        "Issue: The use of generative AI raises ethical concerns, including the potential for creating misleading information, deepfakes, or content that infringes on intellectual property rights.\n",
        "\n",
        "\n",
        "Q.5 Why is Generative AI important for modern applications?\n",
        "\n",
        "Ans :- Generative AI is revolutionizing modern applications by introducing capabilities that enhance efficiency, creativity, and user engagement across various industries. Its importance is underscored by several key factors:\n",
        "\n",
        "Accelerated Content Creation:\n",
        "\n",
        "Efficiency: Generative AI significantly reduces the time and resources required for producing content. For instance, in the fashion industry, AI-driven tools enable rapid design iterations, allowing brands to swiftly adapt to trends and consumer preferences.\n",
        "VOGUE BUSINESS\n",
        "Enhanced Personalization:\n",
        "\n",
        "User Experience: By analyzing user data, generative AI can create personalized content and recommendations, leading to more engaging and tailored experiences. In retail, this translates to customized shopping experiences, such as virtual try-ons and AI-informed product suggestions.\n",
        "\n",
        "\n",
        "Q.6 What is probabilistic modeling in the context of Generative AI?\n",
        "\n",
        "Ans :- Probabilistic modeling in the context of Generative AI involves creating models that represent data distributions using probability theory. These models learn the underlying patterns and structures of data, enabling them to generate new, similar data samples. By estimating the likelihood of different outcomes, probabilistic models can produce diverse and realistic outputs, making them fundamental to Generative AI applications.\n",
        "\n",
        "Key Aspects of Probabilistic Modeling in Generative AI:\n",
        "\n",
        "Learning Data Distributions: Probabilistic models aim to capture the joint probability distribution of input and output variables. This allows them to understand how data points are related and to generate new data that follows the same distribution.\n",
        "TELNYX\n",
        "\n",
        "Generating New Data: Once trained, these models can produce new data samples by sampling from the learned probability distribution. This capability is essential for tasks such as image synthesis, text generation, and other creative applications.\n",
        "\n",
        "Handling Uncertainty: Probabilistic models inherently manage uncertainty and variability in data, making them robust in dealing with real-world scenarios where data can be noisy or incomplete.\n",
        "\n",
        "\n",
        "Q.7 Define a generative model?\n",
        "\n",
        "Ans :- A generative model is a type of machine learning model designed to learn the underlying patterns or distributions within a dataset, enabling it to generate new, similar data samples. Unlike discriminative models, which focus solely on distinguishing between different classes or categories, generative models capture the joint probability distribution of input features and their corresponding labels. This comprehensive understanding allows them to produce realistic data that mirrors the characteristics of the training set.\n",
        "\n",
        "Key Characteristics of Generative Models:\n",
        "\n",
        "Learning Data Distributions: Generative models aim to understand how data is distributed across different dimensions, capturing the probability of observing specific data points.\n",
        "LEARN R, PYTHON & DATA SCIENCE ONLINE\n",
        "\n",
        "Data Generation: Once trained, these models can create new data instances that resemble the training data, making them valuable for tasks such as image synthesis, text generation, and more.\n",
        "\n",
        "\n",
        "Q.8 Explain how an n-gram model works in text generation?\n",
        "\n",
        "Ans :-An n-gram model is a probabilistic language model used in text generation to predict the likelihood of a word based on the preceding sequence of n-1 words. By analyzing the frequency of word sequences (n-grams) in a given corpus, the model estimates the probability of a word occurring after a specific sequence, enabling the generation of coherent and contextually relevant text.\n",
        "\n",
        "How N-Gram Models Work in Text Generation:\n",
        "\n",
        "Corpus Analysis: The model begins by analyzing a large corpus of text to count the occurrences of all possible n-grams. An n-gram is a contiguous sequence of n items (words or characters) from the text. For example, in a trigram model (n=3), the sentence \"I love natural language processing\" contains the trigrams \"I love natural,\" \"love natural language,\" and \"natural language processing.\"\n",
        "\n",
        "Probability Estimation: The model calculates the probability of each word following a given sequence of n-1 words. This is typically done by dividing the frequency of the n-gram by the frequency of the preceding (n-1)-gram. For instance, the probability of the word \"processing\" following the sequence \"natural language\" is estimated as:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "\"processing\"\n",
        "‚à£\n",
        "\"natural¬†language\"\n",
        ")\n",
        "=\n",
        "Count\n",
        "(\n",
        "\"natural¬†language¬†processing\"\n",
        ")\n",
        "Count\n",
        "(\n",
        "\"natural¬†language\"\n",
        ")\n",
        "P(\"processing\"‚à£\"natural¬†language\")=\n",
        "Count(\"natural¬†language\")\n",
        "Count(\"natural¬†language¬†processing\")\n",
        "‚Äã\n",
        "\n",
        "\n",
        "Text Generation: Starting with an initial sequence of n-1 words (often chosen based on context or randomly), the model generates the next word by selecting one with a probability proportional to its estimated likelihood. This new word is then appended to the sequence, and the process repeats, using the most recent n-1 words to predict the next word, until the desired length of text is achieved.\n",
        "\n",
        "\n",
        "# Q.9 What are the limitations of n-gram models\n",
        "\n",
        "Ans :- While n-gram models have been instrumental in natural language processing (NLP), they come with several limitations:\n",
        "\n",
        "Limited Contextual Understanding: N-gram models consider only the immediate n-1 preceding words, which restricts their ability to capture long-range dependencies and contextual nuances in language. This often results in less coherent or contextually appropriate text generation.\n",
        "\n",
        "Data Sparsity: As the value of n increases, the number of possible n-grams grows exponentially, leading to many n-grams with zero or low frequency in the training data. This sparsity can degrade the model's performance, especially when encountering rare or unseen word sequences.\n",
        "\n",
        "Inability to Handle Unseen Words (Out-of-Vocabulary Issues): N-gram models struggle with words or phrases not present in the training corpus, assigning them a probability of zero. This limitation hampers the model's robustness when processing new or evolving language.\n",
        "\n",
        "\n",
        "Q.10 How can you improve the performance of an n-gram model?\n",
        "\n",
        "Ans :- Enhancing the performance of an n-gram model involves several strategies aimed at addressing its inherent limitations:\n",
        "\n",
        "Smoothing Techniques: To mitigate the issue of data sparsity and assign non-zero probabilities to unseen n-grams, various smoothing methods can be employed:\n",
        "\n",
        "Additive (Laplace) Smoothing: Adds a small constant to all n-gram counts, ensuring that no probability is zero.\n",
        "\n",
        "Good-Turing Smoothing: Adjusts the estimated probabilities of n-grams based on the frequency of frequencies, effectively reallocating probability mass to unseen events.\n",
        "\n",
        "Kneser-Ney Smoothing: A more sophisticated method that considers the diversity of contexts in which a word appears, providing better performance, especially for lower-frequency n-grams.\n",
        "\n",
        "Backoff and Interpolation: These techniques allow the model to utilize lower-order n-gram probabilities when higher-order n-grams are sparse or unavailable:\n",
        "\n",
        "Backoff Models: If a higher-order n-gram has a zero count, the model \"backs off\" to a lower-order n-gram to estimate the probability.\n",
        "\n",
        "\n",
        "Q.11 What is the Markov assumption, and how does it apply to text generation?\n",
        "\n",
        "Ans :- The Markov assumption posits that the future state of a system depends solely on its current state, independent of its prior history. In the context of text generation, this means that the probability of a word occurring is determined only by the preceding word(s), not by the entire sequence of words that came before.\n",
        "\n",
        "Application to Text Generation:\n",
        "\n",
        "In text generation, the Markov assumption simplifies the modeling process by focusing on the immediate context. For instance, in a first-order Markov model, the probability of a word is conditioned only on the preceding word. This approach allows for the generation of text that reflects the statistical properties of the training corpus, though it may lack deeper contextual understanding.\n",
        "\n",
        "\n",
        "Q.12 Why are probabilistic models important in generative AI?\n",
        "\n",
        "Ans :- Probabilistic models are fundamental to Generative AI because they provide a structured framework for understanding and generating new data based on learned patterns. By modeling the probability distributions of data, these models can produce new instances that resemble the original data, enabling tasks such as text generation, image synthesis, and music composition.\n",
        "\n",
        "Key Reasons for Their:\n",
        "\n",
        "Capturing Data Distributions: Probabilistic models learn the underlying distributions of data, allowing them to generate new, realistic samples. For example, in text generation, these models can predict the likelihood of a word following a sequence, enabling the creation of coherent and contextually relevant sentences.\n",
        "\n",
        "Handling Uncertainty: These models inherently manage uncertainty, making them robust in scenarios where data is incomplete or noisy. This capability is crucial in real-world applications where perfect information is often unavailable.\n",
        "\n",
        "Flexibility in Data Generation: Probabilistic models can generate diverse outputs from the same input, facilitating creativity and variability in generated content. This flexibility is particularly valuable in fields like art and design, where uniqueness is prized.\n",
        "\n",
        "\n",
        "Q.13 What is an autoencoder?\n",
        "\n",
        "Ans :- An autoencoder is a type of artificial neural network designed to learn efficient representations of unlabeled data through unsupervised learning. It consists of two main components:\n",
        "\n",
        "Encoder: This part compresses the input data into a lower-dimensional latent space, capturing the essential features of the data.\n",
        "\n",
        "Decoder: This component reconstructs the original data from the compressed representation, aiming to minimize the difference between the input and the reconstructed output.\n",
        "\n",
        "The primary objective of an autoencoder is to learn a compact and meaningful representation of the input data, which can be useful for various tasks such as dimensionality reduction, feature extraction, and data denoising.\n",
        "\n",
        "\n",
        "Q.14 How does a VAE differ from a standard autoencoder?\n",
        "\n",
        "Ans :- A Variational Autoencoder (VAE) is a generative model that extends the traditional autoencoder by introducing a probabilistic framework to the latent space. While both models consist of an encoder and a decoder, their approaches to data representation and generation differ significantly.\n",
        "\n",
        "Key Differences Between Autoencoders and Variational Autoencoders:\n",
        "\n",
        "Latent Space Representation:\n",
        "\n",
        "Autoencoder: Maps input data to a fixed point in the latent space, resulting in deterministic encoding.\n",
        "VAE: Maps input data to a distribution in the latent space, typically a multivariate Gaussian, allowing for probabilistic sampling.\n",
        "\n",
        "Autoencoder: Minimizes reconstruction error between input and output.\n",
        "VAE: Minimizes reconstruction error while also regularizing the latent space to follow a specific distribution, promoting smoothness and enabling data generation.\n",
        "\n",
        "Autoencoder: Primarily used for data compression and reconstruction; lacks inherent generative properties.\n",
        "VAE: Designed for data generation; by sampling from the latent space, it can produce new, similar data instances.\n",
        "\n",
        "\n",
        "Q.16 What role does the decoder play in an autoencoder?\n",
        "\n",
        "Ans :- In an autoencoder, the decoder plays a crucial role in reconstructing the original input data from its compressed representation, known as the latent space. This process involves several key functions:\n",
        "\n",
        "Reconstruction of Input Data: The decoder takes the compressed data (latent representation) from the bottleneck layer and reconstructs it back to the original input data‚Äôs dimensions. This reconstruction process involves several steps and utilizes various types of neural network layers to achieve the desired transformation.\n",
        "DEVSHAHS\n",
        "\n",
        "Upsampling and Convolutional Operations: In convolutional autoencoders, the decoder employs upsampling and convolutional blocks to reconstruct the bottleneck's output. Since the input to the decoder is a compressed knowledge representation, the decoder serves as a ‚Äúdecompressor‚Äù and builds back the image from its latent attributes.\n",
        "\n",
        "\n",
        "Q.15 Why are VAEs useful in generative modeling?\n",
        "\n",
        "Ans :- Variational Autoencoders (VAEs) are pivotal in generative modeling due to their ability to learn structured, continuous latent representations of data, facilitating the generation of new, realistic samples. Their significance stems from several key aspects:\n",
        "\n",
        "Probabilistic Framework: VAEs integrate principles from Bayesian inference, enabling the modeling of complex data distributions. This probabilistic approach allows for the generation of new data points by sampling from the learned latent space, ensuring that the generated samples are coherent and plausible.\n",
        "MLQ.AI\n",
        "\n",
        "Regularized Latent Space: By enforcing a regularization term (typically the Kullback-Leibler divergence) during training, VAEs ensure that the latent space follows a predefined distribution, such as a standard normal distribution. This regularization promotes smoothness and continuity in the latent space, which is crucial for generating diverse and realistic samples.\n",
        "\n",
        "Disentangled Representations: VAEs can learn disentangled representations, where each dimension of the latent space captures distinct factors of variation in the data. This property is particularly valuable in applications like image generation, where manipulating specific latent variables can lead to controlled changes in the generated images.\n",
        "\n",
        "\n",
        "Q.17 How does the latent space affect text generation in a VAE?\n",
        "\n",
        "Ans :- In a Variational Autoencoder (VAE), the latent space serves as a compressed, continuous representation of input data, capturing its underlying structure and variations. In the context of text generation, the latent space plays a pivotal role in determining the quality and diversity of the generated text.\n",
        "\n",
        "Impact of Latent Space on Text Generation:\n",
        "\n",
        "Capturing Semantic Features: The latent space encodes essential semantic and syntactic features of the text, enabling the model to generate coherent and contextually relevant sentences. A well-structured latent space allows for the generation of text that mirrors the complexity and diversity of the training data.\n",
        "\n",
        "Controlling Generation: By manipulating points within the latent space, one can guide the generation process to produce text with desired attributes. This control is particularly useful in tasks like style transfer, where the goal is to generate text that maintains the original content but adopts a different style.\n",
        "\n",
        "Ensuring Diversity: A well-structured latent space facilitates the generation of diverse text samples. However, issues like \"latent holes\"‚Äîregions in the latent space that do not correspond to any data points‚Äîcan hinder this diversity, leading to repetitive or nonsensical outputs. Addressing these latent holes is crucial for improving the quality of generated text.\n",
        "\n",
        "\n",
        "Q.18 What is the purpose of the Kullback-Leibler (KL) divergence term in VAEs?\n",
        "\n",
        "Ans :- In Variational Autoencoders (VAEs), the Kullback-Leibler (KL) divergence term plays a crucial role in shaping the latent space to facilitate effective data generation. It measures the divergence between the learned latent distribution and a predefined prior distribution, typically a standard normal distribution. This regularization ensures that the latent space is structured and continuous, allowing for the generation of new, realistic data samples.\n",
        "\n",
        "Key Functions of the KL Divergence Term in VAEs:\n",
        "\n",
        "Regularizing the Latent Space: By penalizing deviations from the prior distribution, the KL divergence term prevents the model from overfitting to the training data. This regularization encourages the encoder to map input data to a latent space that captures the underlying data distribution, promoting generalization to new, unseen data.\n",
        "GEEKSFORGEEKS\n",
        "\n",
        "Enabling Data Generation: A well-structured latent space, achieved through KL divergence regularization, allows for the interpolation between latent variables. This means that by sampling from the latent space, the decoder can generate new data points that are similar to the training data, facilitating tasks such as data augmentation and creative content generation.\n",
        "VINCENT'S BLOG\n",
        "\n",
        "Balancing Reconstruction and Regularization: The VAE's loss function comprises two components: the reconstruction loss and the KL divergence term. The reconstruction loss ensures that the decoder can accurately reconstruct the input data, while the KL divergence term regularizes the latent space. Balancing these components is essential for the model to learn meaningful representations without overfitting.\n",
        "\n",
        "\n",
        "Q.19 How can you prevent overfitting in a VAE?\n",
        "\n",
        "Ans :- Preventing overfitting in Variational Autoencoders (VAEs) is essential to ensure that the model generalizes well to new, unseen data. Overfitting occurs when the model learns to memorize the training data, leading to poor performance on test data. To mitigate this, several strategies can be employed:\n",
        "\n",
        "Regularization of the Latent Space:\n",
        "\n",
        "Kullback-Leibler (KL) Divergence Term: The KL divergence term in the VAE loss function measures the difference between the learned latent distribution and a predefined prior distribution, typically a standard normal distribution. By penalizing deviations from this prior, the KL divergence term regularizes the latent space, encouraging the model to learn a structured and continuous representation. This regularization helps prevent overfitting by ensuring that the latent space captures the underlying data distribution without memorizing specific data points.\n",
        "WIKIPEDIA\n",
        "Denoising Autoencoder Approach:\n",
        "\n",
        "Input Corruption: Introducing noise or corruption to the input data during training forces the model to learn robust features that are invariant to such perturbations. This approach enhances the model's ability to generalize and reduces the risk of overfitting.\n",
        "\n",
        "\n",
        "Q.20 Explain why VAEs are commonly used for unsupervised learning tasks?\n",
        "\n",
        "Ans :- Variational Autoencoders (VAEs) are widely used for unsupervised learning tasks due to their ability to learn meaningful, compressed representations of data without the need for labeled examples. This capability stems from several key features:\n",
        "\n",
        "Generative Modeling: VAEs are designed to model the underlying distribution of input data, enabling them to generate new, similar data samples. This generative aspect is particularly useful in tasks like data augmentation, where creating new data points can enhance the robustness of machine learning models.\n",
        "\n",
        "Dimensionality Reduction: By learning a compressed latent representation, VAEs effectively reduce the dimensionality of data. This reduction simplifies complex data structures, making it easier to identify patterns and relationships within the data.\n",
        "\n",
        "Anomaly Detection: VAEs can identify data points that deviate from the learned distribution, making them effective for anomaly detection tasks. By reconstructing input data and measuring reconstruction errors, VAEs can flag instances that are significantly different from the norm.\n",
        "PMC\n",
        "\n",
        "Clustering and Feature Learning: The structured latent space learned by VAEs can facilitate clustering and feature extraction, aiding in the discovery of inherent groupings within the data. This property is beneficial in exploratory data analysis and pattern recognition tasks.\n",
        "\n",
        "\n",
        "Q.21 What is a transformer model?\n",
        "\n",
        "Ans :- A transformer model is a deep learning architecture introduced in 2017 that has revolutionized the field of natural language processing (NLP) and beyond. Unlike traditional recurrent neural networks (RNNs) and long short-term memory networks (LSTMs), transformers utilize a mechanism called self-attention to process input data in parallel, allowing them to capture complex relationships within sequences more efficiently.\n",
        "\n",
        "Key Components of Transformer Models:\n",
        "\n",
        "Self-Attention Mechanism: This mechanism enables the model to weigh the importance of different parts of the input data relative to each other. By assigning varying attention scores, the model can focus on relevant information, regardless of its position in the sequence.\n",
        "\n",
        "Multi-Head Attention: Transformers employ multiple attention heads to capture different aspects of the relationships within the data. Each head learns to focus on different parts of the input, providing a richer understanding of the context\n",
        "\n",
        "\n",
        "Q.22 Explain the purpose of self-attention in transformers?\n",
        "\n",
        "Ans :- In transformer models, self-attention is a mechanism that enables each element of an input sequence to dynamically focus on different parts of the same sequence. This capability allows the model to capture complex, long-range dependencies within the data, which is particularly beneficial for tasks involving sequential information, such as natural language processing.\n",
        "\n",
        "How Self-Attention Works:\n",
        "\n",
        "Query, Key, and Value Vectors: Each input element is transformed into three vectors: a query, a key, and a value. These vectors are derived through learned linear transformations of the input embeddings.\n",
        "\n",
        "Attention Scores: The model computes attention scores by taking the dot product of the query vector of one element with the key vectors of all elements in the sequence. These scores determine the relevance of each element to the others.\n",
        "\n",
        "Softmax Normalization: The attention scores are passed through a softmax function to normalize them into a probability distribution, ensuring that the weights assigned to each value vector sum to one.\n",
        "\n",
        "\n",
        "Q.23 How does a GPT model generate text?\n",
        "\n",
        "Ans :- Generative Pre-trained Transformer (GPT) models, such as GPT-3, generate text through a process that involves understanding and predicting language patterns. Here's how they work:\n",
        "\n",
        "Pre-training: GPT models are initially trained on vast amounts of text data to learn the statistical relationships between words and phrases. This training enables the model to understand context, grammar, and the structure of language.\n",
        "\n",
        "Input Processing: When a prompt is provided, the model tokenizes the input text into smaller units, such as words or subwords, converting them into numerical representations.\n",
        "\n",
        "Contextual Understanding: Using the self-attention mechanism, the model analyzes the relationships between tokens in the input sequence, allowing it to capture long-range dependencies and contextual information.\n",
        "\n",
        "Text Generation: Based on the input and its learned knowledge, the model predicts the next token in the sequence. This prediction is probabilistic, considering the likelihood of various possible next tokens. The model then appends the predicted token to the input and repeats the process iteratively to generate coherent and contextually relevant text\n",
        "\n",
        "\n",
        "Q.24 What are the key differences between a GPT model and an RNN?\n",
        "\n",
        "Ans :- Generative Pre-trained Transformer (GPT) models and Recurrent Neural Networks (RNNs) are both designed for processing sequential data, but they differ significantly in architecture, processing mechanisms, and performance.\n",
        "\n",
        "1. Architecture and Processing Mechanism:\n",
        "\n",
        "RNNs: Recurrent Neural Networks process sequences by maintaining a hidden state that captures information from previous time steps. This design allows RNNs to handle sequential data but can lead to challenges in capturing long-range dependencies due to issues like vanishing gradients.\n",
        "\n",
        "GPT Models: GPT models utilize the Transformer architecture, which employs self-attention mechanisms to process all elements of a sequence simultaneously. This parallel processing capability enables GPT models to capture complex relationships within data more efficiently.\n",
        "\n",
        "\n",
        "Q.25 How does fine-tuning improve a pre-trained GPT model?\n",
        "\n",
        "Ans :- Fine-tuning enhances a pre-trained Generative Pre-trained Transformer (GPT) model by adapting it to specific tasks or domains, thereby improving its performance and relevance. Here's how fine-tuning achieves this:\n",
        "\n",
        "1. Domain-Specific Adaptation:\n",
        "\n",
        "Fine-tuning involves training the pre-trained GPT model on a specialized dataset pertinent to a particular domain or task. This process allows the model to learn domain-specific terminology, context, and nuances, enabling it to generate more accurate and contextually appropriate responses.\n",
        "LEARN R, PYTHON & DATA SCIENCE ONLINE\n",
        "\n",
        "2. Improved Task Performance:\n",
        "\n",
        "By exposing the model to examples of the desired output during fine-tuning, it learns to produce responses that align more closely with the specific requirements of the task. This targeted training enhances the model's ability to handle specialized tasks effectively.\n",
        "\n",
        "\n",
        "Q.26 What is zero-shot learning in the context of GPT models?\n",
        "\n",
        "Ans :- Zero-shot learning enables Generative Pre-trained Transformer (GPT) models to perform tasks without explicit examples or additional training for those specific tasks. Instead, these models leverage their extensive pre-training on diverse text data to understand and execute new tasks based on natural language instructions.\n",
        "\n",
        "How Zero-Shot Learning Works in GPT Models:\n",
        "\n",
        "Pre-training on Diverse Data: GPT models are trained on large, varied text corpora, allowing them to acquire a broad understanding of language, grammar, and general knowledge.\n",
        "\n",
        "Task Interpretation via Prompts: When presented with a new task, GPT models interpret the instructions provided in the prompt. The prompt serves as a guide, directing the model to apply its pre-existing knowledge to the new task.\n",
        "\n",
        "Generating Responses: Based on the prompt, the model generates responses by predicting the most likely next words or phrases, effectively performing the task without additional training.\n",
        "\n",
        "\n",
        "Q.27 Describe how prompt engineering can impact GPT model performanc?\n",
        "\n",
        "Ans :- Prompt engineering involves crafting input prompts to guide Generative Pre-trained Transformer (GPT) models in producing desired outputs. By carefully designing prompts, users can significantly influence the model's performance, enhancing the relevance and accuracy of its responses.\n",
        "\n",
        "Impact of Prompt Engineering on GPT Model Performance:\n",
        "\n",
        "Enhanced Accuracy and Relevance: Well-structured prompts provide clear context and instructions, enabling the model to generate more precise and contextually appropriate responses. This alignment with user expectations leads to improved performance in tasks such as text generation, summarization, and translation.\n",
        "CIRCLECI\n",
        "\n",
        "Improved Task Alignment: By specifying the task within the prompt, users can guide the model to focus on the desired outcome, whether it's answering a question, completing a sentence, or performing a specific analysis. This targeted approach ensures that the model's outputs are aligned with the intended task.\n",
        "OPENAI PLATFORM\n",
        "\n",
        "Reduced Ambiguity: Clear and unambiguous prompts help minimize the model's reliance on assumptions, leading to more accurate and reliable outputs. This clarity is particularly important in complex tasks where precision is crucial.\n",
        "SYNOPTEK\n",
        "\n",
        "Efficient Resource Utilization: Effective prompt engineering can reduce the need for extensive fine-tuning or retraining of the model, saving computational resources and time. By optimizing prompts, users can achieve desired results without the overhead of additional training.\n",
        "\n",
        "\n",
        "Q.28 Why are large datasets essential for training GPT models?\n",
        "\n",
        "Ans :- Large datasets are essential for training Generative Pre-trained Transformer (GPT) models due to several key reasons:\n",
        "\n",
        "Comprehensive Language Understanding: Training on extensive and diverse text corpora enables GPT models to capture a wide range of linguistic patterns, nuances, and contextual relationships. This broad exposure allows the model to generate coherent and contextually appropriate responses across various topics and domains.\n",
        "\n",
        "Improved Generalization: Access to large datasets helps GPT models generalize better to new, unseen data. By learning from a vast array of examples, the model can apply its knowledge to different contexts, enhancing its versatility and robustness in generating human-like text.\n",
        "\n",
        "\n",
        "Q.29 What are potential ethical concerns with GPT models?\n",
        "\n",
        "Ans :- Generative Pre-trained Transformer (GPT) models have significantly advanced natural language processing, but their deployment raises several ethical concerns:\n",
        "\n",
        "Bias and Discrimination: GPT models trained on large datasets may inadvertently learn and reproduce societal biases present in the data, leading to outputs that reinforce stereotypes or discriminatory views.\n",
        "MDPI\n",
        "\n",
        "Misinformation and Disinformation: These models can generate plausible-sounding but false information, posing risks when used to spread misinformation or disinformation intentionally or unintentionally.\n",
        "AICONTENTFY\n",
        "\n",
        "Privacy Concerns: Training data may include personal or sensitive information, raising concerns about privacy breaches if the model inadvertently reproduces such data in its outputs.\n",
        "\n",
        "\n",
        "Q.30 How does the attention mechanism contribute to GPT‚Äôs ability to handle\n",
        "long-range dependencies?\n",
        "\n",
        "Ans :- The attention mechanism is fundamental to GPT models, enabling them to effectively manage long-range dependencies in text. Here's how it contributes:\n",
        "\n",
        "1. Contextual Weighting:\n",
        "\n",
        "The attention mechanism assigns varying levels of importance to different words in a sequence, allowing the model to focus on relevant words regardless of their position. This capability is crucial for understanding and generating coherent text, as it captures relationships between distant words.\n",
        "LEARN R, PYTHON & DATA SCIENCE ONLINE\n",
        "\n",
        "2. Parallel Processing:\n",
        "\n",
        "Unlike traditional sequential models, the attention mechanism processes all words in a sequence simultaneously. This parallelism enables the model to capture complex patterns and dependencies more efficiently, enhancing its ability to handle long-range relationships.\n",
        "\n",
        "\n",
        "Q.31 What are some limitations of GPT models for real-world applications?\n",
        "\n",
        "Ans :- Generative Pre-trained Transformer (GPT) models have significantly advanced natural language processing, yet they face several limitations in real-world applications:\n",
        "\n",
        "Inaccuracies and Hallucinations: GPT models can produce plausible-sounding but incorrect or nonsensical answers, a phenomenon known as \"hallucination.\" This issue necessitates human oversight to ensure the reliability of the information generated.\n",
        "\n",
        "Contextual and Memory Constraints: These models may struggle with maintaining context over extended conversations or documents, leading to responses that are off-topic or inconsistent.\n",
        "\n",
        "Dependence on Training Data: GPT models rely heavily on their training data and may not effectively incorporate new information in real-time, limiting their ability to provide up-to-date responses.\n",
        "\n",
        "\n",
        "Q.32 How can GPT models be adapted for domain-specific text generation?\n",
        "\n",
        "Ans :- Adapting GPT models for domain-specific text generation enhances their relevance and accuracy in specialized fields. Key strategies include:\n",
        "\n",
        "1. Fine-Tuning with Domain-Specific Data:\n",
        "\n",
        "Fine-tuning involves retraining a pre-trained GPT model on a dataset tailored to the target domain. This process enables the model to learn the unique language patterns and terminologies of the specific field, improving its performance in generating relevant content.\n",
        "BLOCKCHAIN DEV CO. USA & CANADA\n",
        "\n",
        "2. Prompt Engineering:\n",
        "\n",
        "Crafting precise and context-rich prompts guides the model to produce outputs aligned with domain-specific requirements. Effective prompt engineering can adapt the model's behavior without extensive retraining, making it a practical approach for domain adaptation.\n",
        "\n",
        "3. Combining Fine-Tuning and Prompt Engineering:\n",
        "\n",
        "Integrating both techniques can yield superior results. Fine-tuning adjusts the model to the domain's general context, while prompt engineering directs it to generate specific outputs, enhancing overall performance in specialized tasks.\n",
        "\n",
        "\n",
        "Q.33 What are some common metrics for evaluating text generation quality?\n",
        "\n",
        "Ans :-Evaluating the quality of text generated by models is crucial to ensure coherence, relevance, and fluency. Common metrics used for this purpose include:\n",
        "\n",
        "BLEU (Bilingual Evaluation Understudy): Measures the overlap between n-grams in the generated text and reference text, focusing on precision. Commonly used in machine translation and text summarization tasks.\n",
        "TOWARDS DATA SCIENCE\n",
        "\n",
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation): Assesses the overlap of n-grams, particularly favoring recall, between the generated and reference texts. Frequently applied in evaluating summarization models.\n",
        "TOWARDS DATA SCIENCE\n",
        "\n",
        "METEOR (Metric for Evaluation of Translation with Explicit ORdering): Considers precision and recall, along with stemming and synonymy, to evaluate the alignment between generated and reference texts. Designed to address some limitations of BLEU.\n",
        "\n",
        "Q.34 Explain the difference between deterministic and probabilistic text\n",
        "generation?\n",
        "\n",
        "Ans :- In text generation, deterministic and probabilistic methods define how models select the next word or token in a sequence:\n",
        "\n",
        "Deterministic Text Generation:\n",
        "\n",
        "Definition: Utilizes fixed rules to select the next token, ensuring consistent outputs for identical inputs.\n",
        "\n",
        "Methods:\n",
        "\n",
        "Greedy Search: Chooses the token with the highest probability at each step, which can lead to repetitive or predictable sequences.\n",
        "MEDIUM\n",
        "\n",
        "Beam Search: Maintains multiple candidate sequences, expanding the most probable ones iteratively. While it can produce better quality text, it may still result in repetitive outputs and is computationally more intensive.\n",
        "MEDIUM\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Predictable and repeatable outputs.\n",
        "\n",
        "May lack diversity and creativity in generated text.\n",
        "\n",
        "\n",
        "Q.35 How does beam search improve text generation in language models?\n",
        "\n",
        "Ans :- Beam search is a decoding algorithm that enhances text generation in language models by exploring multiple potential sequences simultaneously, thereby improving the quality and coherence of the generated output.\n",
        "\n",
        "How Beam Search Works:\n",
        "\n",
        "Unlike greedy search, which selects the highest-probability token at each step, beam search maintains multiple candidate sequences (beams) throughout the generation process. The algorithm operates as follows:\n",
        "\n",
        "Initialization: Start with the initial token and create multiple beams, each representing a potential sequence.\n",
        "\n",
        "Expansion: At each step, expand each beam by generating all possible next tokens and calculate their cumulative probabilities.\n",
        "\n",
        "Pruning: Select the top 'k' beams with the highest cumulative probabilities, where 'k' is the beam width, and discard the rest."
      ],
      "metadata": {
        "id": "tda0N0x7ZRtU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical Questions"
      ],
      "metadata": {
        "id": "wKzbqKxarGDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.1  Write a code to generate a random sentence using probabilistic modeling (Markov Chain). Use the sentence \"The cat is on the mat\" as an exampl@\n",
        "# Ans :-\n",
        "pip install markovify\n",
        "import markovify\n",
        "\n",
        "# Define the example sentence\n",
        "text = \"The cat is on the mat.\"\n",
        "\n",
        "# Build the Markov model\n",
        "text_model = markovify.Text(text, state_size=1)\n",
        "\n",
        "# Generate a random sentence\n",
        "generated_sentence = text_model.make_sentence()\n",
        "\n",
        "print(generated_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "lrQ-AakOZKv9",
        "outputId": "483c0086-0923-4301-d210-8867bd231f36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-b8c0c28f7eff>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-b8c0c28f7eff>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    pip install markovify\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8bVe05XYVsJ",
        "outputId": "71c0aca2-3379-459f-eb81-041928904c3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        }
      ],
      "source": [
        "# Q.2 I Build a simple Autoencoder model using Keras to learn a compressed representation of a given sentence. Use a dataset of your choice\n",
        "# Ans :-\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, RepeatVector, TimeDistributed\n",
        "\n",
        "# Parameters\n",
        "max_features = 10000  # Vocabulary size\n",
        "maxlen = 100          # Maximum sequence length\n",
        "embedding_dim = 128   # Embedding dimensions\n",
        "latent_dim = 64       # Latent space dimension\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, _), (x_test, _) = imdb.load_data(num_words=max_features)\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=(maxlen,))\n",
        "embedding_layer = Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=maxlen)(inputs)\n",
        "encoded = LSTM(latent_dim)(embedding_layer)\n",
        "\n",
        "# Decoder\n",
        "decoded = RepeatVector(maxlen)(encoded)\n",
        "decoded = LSTM(embedding_dim, return_sequences=True)(decoded)\n",
        "outputs = TimeDistributed(Dense(max_features, activation='softmax'))(decoded)\n",
        "\n",
        "# Autoencoder Model\n",
        "autoencoder = Model(inputs, outputs)\n",
        "autoencoder.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Data reshaping for sparse_categorical_crossentropy\n",
        "x_train_reshaped = np.expand_dims(x_train, -1)\n",
        "x_test_reshaped = np.expand_dims(x_test, -1)\n",
        "\n",
        "# Training\n",
        "autoencoder.fit(x_train, x_train_reshaped,\n",
        "                epochs=10,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test_reshaped))\n",
        "\n",
        "# Encoder Model for extracting latent representations\n",
        "encoder = Model(inputs, encoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.3 Use the Hugging Face transformers library to fine-tune a pre-trained GPT-2 model on a custom text data and generate text\n",
        "# Ans :-\n",
        "!pip install transformers datasets\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load your custom dataset\n",
        "dataset = load_dataset('text', data_files={'train': 'path_to_your_dataset.txt'})\n",
        "\n",
        "# Load the GPT-2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Load the pre-trained GPT-2 model\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_steps=10,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='loss',\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['train'],  # Using train as eval for simplicity\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "# Function to generate text\n",
        "def generate_text(prompt, max_length=50):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Example usage\n",
        "prompt = \"Once upon a time\"\n",
        "generated_text = generate_text(prompt)\n",
        "print(generated_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "b15Uay4vss6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.4 I Implement a text generation model using a simple Recurrent Neural Network (RNN) in Keras. Train the model on a custom data and generate a word\n",
        "# Ans :-\n",
        "!pip install tensorflow numpy\n",
        "import numpy as np\n",
        "\n",
        "# Load your custom dataset\n",
        "with open('path_to_your_dataset.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read().lower()\n",
        "\n",
        "# Create a set of unique characters\n",
        "chars = sorted(set(text))\n",
        "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
        "\n",
        "# Define sequence length and step\n",
        "seq_length = 100\n",
        "step = 1\n",
        "\n",
        "# Prepare sequences and their corresponding next characters\n",
        "sequences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - seq_length, step):\n",
        "    sequences.append(text[i:i + seq_length])\n",
        "    next_chars.append(text[i + seq_length])\n",
        "\n",
        "# Vectorize the sequences and next characters\n",
        "X = np.zeros((len(sequences), seq_length, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
        "for i, seq in enumerate(sequences):\n",
        "    for t, char in enumerate(seq):\n",
        "        X[i, t, char_to_idx[char]] = 1\n",
        "    y[i, char_to_idx[next_chars[i]]] = 1\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(128, input_shape=(seq_length, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=128, epochs=10)\n",
        "def generate_text(model, seed, length=500):\n",
        "    generated = seed\n",
        "    for _ in range(length):\n",
        "        # Vectorize the seed\n",
        "        x_pred = np.zeros((1, len(seed), len(chars)))\n",
        "        for t, char in enumerate(seed):\n",
        "            x_pred[0, t, char_to_idx[char]] = 1\n",
        "\n",
        "        # Predict the next character\n",
        "        preds = model.predict(x_pred, verbose=0)\n",
        "        next_idx = np.argmax(preds)\n",
        "        next_char = idx_to_char[next_idx]\n",
        "\n",
        "        # Append the predicted character to the generated text\n",
        "        generated += next_char\n",
        "        seed = seed[1:] + next_char\n",
        "    return generated\n",
        "\n",
        "# Example usage\n",
        "seed_text = \"Once upon a time\"\n",
        "generated_text = generate_text(model, seed_text)\n",
        "print(generated_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "6_6OGpYAtqw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.5 Write a program to generate a sequence of text using an LSTM-based model in TensorFlow, trained on a custom data of sentences\n",
        "# Ans :-\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Step 1: Load and prepare the data\n",
        "data = [\n",
        "    \"This is the first sentence.\",\n",
        "    \"Here is another sentence.\",\n",
        "    \"Text generation using LSTM is fun.\",\n",
        "    \"We are learning how to generate text.\",\n",
        "    \"This model is trained to generate sequences.\"\n",
        "]\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data)\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "\n",
        "# Prepare input and output sequences for training\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for seq in sequences:\n",
        "    for i in range(1, len(seq)):\n",
        "        X.append(seq[:i])\n",
        "        y.append(seq[i])\n",
        "\n",
        "# Pad sequences to ensure they are of equal length\n",
        "max_sequence_length = max(len(seq) for seq in X)\n",
        "X = pad_sequences(X, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "# Convert y to one-hot encoded vectors\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Step 2: Build the LSTM Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=50, input_length=max_sequence_length))\n",
        "model.add(LSTM(128, return_sequences=False))\n",
        "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Step 3: Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=32)\n",
        "\n",
        "# Step 4: Generate text\n",
        "def generate_text(seed_text, num_words):\n",
        "    for _ in range(num_words):\n",
        "        # Convert seed_text to sequence\n",
        "        sequence = tokenizer.texts_to_sequences([seed_text])\n",
        "        sequence = pad_sequences(sequence, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "        # Predict the next word\n",
        "        predicted_probs = model.predict(sequence, verbose=0)\n",
        "        predicted_word_index = np.argmax(predicted_probs)\n",
        "\n",
        "        # Get the word corresponding to the predicted index\n",
        "        predicted_word = tokenizer.index_word.get(predicted_word_index, '')\n",
        "\n",
        "        # Append the predicted word to the seed_text\n",
        "        seed_text += ' ' + predicted_word\n",
        "\n",
        "    return seed_text\n",
        "\n",
        "# Example usage\n",
        "seed_text = \"This is\"\n",
        "generated_text = generate_text(seed_text, 5)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "R9qq97faugUj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd24535c-f94a-4eb0-f2c0-0bb20090952d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.0000e+00 - loss: 3.0908\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380ms/step - accuracy: 0.2500 - loss: 3.0842\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - accuracy: 0.1667 - loss: 3.0775\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.1667 - loss: 3.0705\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.1667 - loss: 3.0630\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.1667 - loss: 3.0548\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.1667 - loss: 3.0456\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.1667 - loss: 3.0351\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.1667 - loss: 3.0230\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.1667 - loss: 3.0090\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.1667 - loss: 2.9926\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.1667 - loss: 2.9735\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - accuracy: 0.1667 - loss: 2.9513\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.1667 - loss: 2.9256\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.1667 - loss: 2.8966\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.1667 - loss: 2.8652\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.1667 - loss: 2.8344\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.1667 - loss: 2.8101\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.1667 - loss: 2.7997\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - accuracy: 0.1667 - loss: 2.7982\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.1667 - loss: 2.7877\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.1667 - loss: 2.7645\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.1667 - loss: 2.7364\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.1667 - loss: 2.7109\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step - accuracy: 0.1667 - loss: 2.6903\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.1667 - loss: 2.6729\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297ms/step - accuracy: 0.1667 - loss: 2.6562\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.2083 - loss: 2.6381\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.2500 - loss: 2.6171\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.2500 - loss: 2.5930\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.2500 - loss: 2.5659\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step - accuracy: 0.2500 - loss: 2.5366\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.2917 - loss: 2.5060\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.2917 - loss: 2.4751\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.2917 - loss: 2.4441\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.2917 - loss: 2.4120\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.3750 - loss: 2.3770\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.3750 - loss: 2.3379\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.4167 - loss: 2.2953\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.4167 - loss: 2.2508\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.4167 - loss: 2.2056\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.4167 - loss: 2.1595\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.4167 - loss: 2.1114\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.4167 - loss: 2.0614\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step - accuracy: 0.4167 - loss: 2.0116\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.4167 - loss: 1.9640\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.4167 - loss: 1.9174\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - accuracy: 0.4167 - loss: 1.8710\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.4167 - loss: 1.8269\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 509ms/step - accuracy: 0.4167 - loss: 1.7844\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306ms/step - accuracy: 0.4583 - loss: 1.7412\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.4583 - loss: 1.6995\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.4583 - loss: 1.6580\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.5000 - loss: 1.6159\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.5000 - loss: 1.5757\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.5000 - loss: 1.5344\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.5000 - loss: 1.4944\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.5000 - loss: 1.4533\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.5000 - loss: 1.4133\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step - accuracy: 0.5833 - loss: 1.3729\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step - accuracy: 0.5833 - loss: 1.3339\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - accuracy: 0.6250 - loss: 1.2944\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.6250 - loss: 1.2557\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step - accuracy: 0.6667 - loss: 1.2162\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.6667 - loss: 1.1771\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step - accuracy: 0.6667 - loss: 1.1395\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.6667 - loss: 1.1019\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - accuracy: 0.6667 - loss: 1.0652\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.6667 - loss: 1.0290\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.7083 - loss: 0.9929\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.7500 - loss: 0.9572\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.7500 - loss: 0.9227\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.7500 - loss: 0.8894\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.7500 - loss: 0.8579\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.7917 - loss: 0.8285\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.7917 - loss: 0.8013\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step - accuracy: 0.8333 - loss: 0.7735\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step - accuracy: 0.8333 - loss: 0.7426\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8333 - loss: 0.7172\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - accuracy: 0.9167 - loss: 0.6955\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8750 - loss: 0.6698\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9167 - loss: 0.6452\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.9583 - loss: 0.6255\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.9167 - loss: 0.6054\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.9583 - loss: 0.5838\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9583 - loss: 0.5639\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9167 - loss: 0.5472\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.9583 - loss: 0.5308\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.9583 - loss: 0.5127\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - accuracy: 0.9583 - loss: 0.4959\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9583 - loss: 0.4812\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.9583 - loss: 0.4676\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - accuracy: 0.9583 - loss: 0.4546\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.9583 - loss: 0.4399\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9583 - loss: 0.4259\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.9583 - loss: 0.4133\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.9583 - loss: 0.4019\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - accuracy: 0.9583 - loss: 0.3914\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9583 - loss: 0.3802\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.9583 - loss: 0.3690\n",
            "This is the first sentence generate sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "epf6NvCavWXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.6 I Build a program that uses GPT-2 from Hugging Face to generate a story based on a custom prompt\n",
        "# Ans :-\n",
        "!pip install transformers torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Step 1: Load the pre-trained GPT-2 model and tokenizer from Hugging Face\n",
        "model_name = \"gpt2\"  # You can use 'gpt2-medium', 'gpt2-large', or 'gpt2-xl' for larger models\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Step 2: Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Step 3: Define a function to generate text based on a custom prompt\n",
        "def generate_story(prompt, max_length=100):\n",
        "    # Encode the prompt text and convert it to tensor\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate text from the model using the prompt\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,  # Return one story\n",
        "            no_repeat_ngram_size=2,  # Prevent repetition\n",
        "            top_k=50,  # Top-k sampling for more diverse results\n",
        "            top_p=0.95,  # Nucleus sampling for diversity\n",
        "            temperature=0.7,  # Control randomness\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode the output sequence and return the generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Example usage\n",
        "prompt = \"Once upon a time in a faraway kingdom, there was a young prince who\"\n",
        "generated_story = generate_story(prompt, max_length=200)\n",
        "\n",
        "print(\"Generated Story:\")\n",
        "print(generated_story)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq7feRf69ct_",
        "outputId": "3b6499a0-ced7-4743-8d61-2dfb04513623"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Generated Story:\n",
            "Once upon a time in a faraway kingdom, there was a young prince who was the son of a nobleman. He was called the Prince of the Kings, and he was known as the King of Kings. The prince was named Prince de la Ville, because he had been the first to become king of France.\n",
            "\n",
            "The prince had a great deal of power, but he did not have the strength to fight. His father, the Duke of Orleans, was an old man, who had died in the war. When he came to France, he found that he could not fight, for he knew that the king was dead. So he went to the prince and said, \"I will fight you, my son, if you will not give me your sword.\" The king said to him, 'I have no sword, I will give you my sword.' So the young man went and fought him. Then the old prince said: \"You have not the sword? You have been\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.7 Write a code to implement a simple text generation model using a GRUÔøæbased architecture in Keras?\n",
        "# Ans :-\n",
        "pip install tensorflow\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Step 1: Load and preprocess the data\n",
        "text = \"\"\"Your custom text data here. You can replace this with any text corpus\n",
        "           for training the text generation model. You can use books, articles, or any text data.\"\"\"\n",
        "\n",
        "# Step 2: Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1  # Add 1 for the padding token\n",
        "\n",
        "# Step 3: Create input sequences and labels\n",
        "sequences = []\n",
        "for i in range(1, len(text.split())):\n",
        "    # Generate sequences of words\n",
        "    sequence = text.split()[:i+1]\n",
        "    sequences.append(tokenizer.texts_to_sequences([sequence])[0])\n",
        "\n",
        "# Step 4: Pad sequences to ensure uniform length\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "X = [seq[:-1] for seq in sequences]  # Input sequences\n",
        "y = [seq[-1] for seq in sequences]   # Output labels\n",
        "\n",
        "X = pad_sequences(X, maxlen=max_sequence_length-1, padding='pre')\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
        "\n",
        "# Step 5: Build the GRU-based model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 50, input_length=X.shape[1]))\n",
        "model.add(GRU(128, return_sequences=False))  # GRU layer with 128 units\n",
        "model.add(Dense(total_words, activation='softmax'))  # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Step 6: Train the model\n",
        "model.fit(X, y, epochs=50, batch_size=64)\n",
        "\n",
        "# Step 7: Generate text\n",
        "def generate_text(seed_text, next_words, model, tokenizer, max_sequence_length):\n",
        "    for _ in range(next_words):\n",
        "        # Encode the seed text and pad the sequence\n",
        "        tokenized_seed = tokenizer.texts_to_sequences([seed_text])\n",
        "        tokenized_seed = pad_sequences(tokenized_seed, maxlen=max_sequence_length-1, padding='pre')\n",
        "\n",
        "        # Predict the next word\n",
        "        predicted_probs = model.predict(tokenized_seed, verbose=0)\n",
        "        predicted_word_index = np.argmax(predicted_probs)\n",
        "\n",
        "        # Get the word corresponding to the predicted index\n",
        "        predicted_word = tokenizer.index_word.get(predicted_word_index, '')\n",
        "\n",
        "        # Append the predicted word to the seed text\n",
        "        seed_text += ' ' + predicted_word\n",
        "\n",
        "    return seed_text\n",
        "\n",
        "# Example usage\n",
        "seed_text = \"Your starting text\"\n",
        "generated_text = generate_text(seed_text, next_words=50, model=model, tokenizer=tokenizer, max_sequence_length=max_sequence_length)\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "q7ECRoRx9cd9",
        "outputId": "a685ad26-4e26-4687-8bc1-a4bba106770b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-3-36b359074342>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-36b359074342>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    pip install tensorflow\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.8 I Create a script to implement GPT-2-based text generation with beam search decoding to generate text\n",
        "# Ans :-\n",
        "!pip install transformers torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"  # You can use 'gpt2-medium', 'gpt2-large', or 'gpt2-xl' for larger models\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Beam search function\n",
        "def generate_text_with_beam_search(prompt, beam_size=5, max_length=100):\n",
        "    # Encode the input prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "    # Generate text using beam search\n",
        "    beam_output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_beams=beam_size,\n",
        "        no_repeat_ngram_size=2,  # Prevent repeating n-grams\n",
        "        early_stopping=True,\n",
        "        top_k=50,  # Optional: Control randomness\n",
        "        top_p=0.95,  # Optional: Control diversity (nucleus sampling)\n",
        "        temperature=0.7,  # Optional: Control randomness\n",
        "    )\n",
        "\n",
        "    # Decode the generated output\n",
        "    generated_text = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Example usage\n",
        "prompt = \"Once upon a time in a distant galaxy\"\n",
        "generated_story = generate_text_with_beam_search(prompt, beam_size=5, max_length=200)\n",
        "\n",
        "print(\"Generated Story:\")\n",
        "print(generated_story)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AImazVk_-APv",
        "outputId": "033c48c9-50fa-4395-c8d3-b6db4944cdf1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Story:\n",
            "Once upon a time in a distant galaxy, a group of intelligent beings, known as the Guardians of the Galaxy, came together to create a new form of life.\n",
            "\n",
            "The Guardians were the first of their kind to appear in the Marvel Universe. They were created by Captain Marvel and his team of super-villains to protect the galaxy from the forces of evil. However, they were defeated by Thanos, who used them as a weapon to destroy the Avengers. The Guardians, along with the rest of Marvel's heroes and villains, were forced to leave the planet and return to Earth in order to save the universe from destruction. In the aftermath of this event, Captain America, Iron Man, Spider-Man, Black Widow, and Hawkeye were all killed in an action-packed battle to the death. After the events of Avengers: Age of Ultron, it was revealed that the original Guardians had been killed, but that they had survived the battle and were now living in their own universe.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.9 I Implement a text generation script using GPT-2 with a custom temperature setting for diversity in output text\n",
        "# Ans:-\n",
        "!pip install transformers torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"  # You can also try 'gpt2-medium', 'gpt2-large', or 'gpt2-xl' for larger models\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Function to generate text with a custom temperature setting\n",
        "def generate_text_with_temperature(prompt, temperature=1.0, max_length=100):\n",
        "    # Encode the input prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "    # Generate text with the given temperature setting\n",
        "    generated_output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,  # Custom temperature setting for diversity\n",
        "        top_k=50,  # Optional: Limit the sampling pool to top-k words\n",
        "        top_p=0.95,  # Optional: Nucleus sampling for diversity\n",
        "        no_repeat_ngram_size=2,  # Prevent repeating n-grams\n",
        "        early_stopping=True,\n",
        "        num_return_sequences=1  # Generate one sequence\n",
        "    )\n",
        "\n",
        "    # Decode the generated output and return as text\n",
        "    generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Example usage\n",
        "prompt = \"In the year 3025, humanity had reached the stars and\"\n",
        "generated_text = generate_text_with_temperature(prompt, temperature=1.2, max_length=150)\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlYCUHXK-Qzn",
        "outputId": "c9d43f9a-eada-4c2d-fb9d-def757f6a7f0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "In the year 3025, humanity had reached the stars and the planet was in the process of being destroyed. The planet had been destroyed by the forces of the Galactic Empire, and was now in a state of war with the other planets.\n",
            "\n",
            "The Galactic Republic had taken control of Earth and had begun to destroy the planets in order to prevent the destruction of their systems. However, the Empire had also taken over the Earth, which had become a major threat to the Republic. In order for the Jedi to be able to defend the galaxy, they had to defeat the Sith. This was accomplished by using the Force to create a new Sith Empire. As the new Empire was created, it was able, through the use of Force powers, to control\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.10 Create a script to implement temperature sampling with GPT-2, experimenting with different values to generate creative text\n",
        "# Ans :-\n",
        "!pip install transformers torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load the pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"  # You can experiment with 'gpt2-medium', 'gpt2-large', or 'gpt2-xl'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Function to generate text using temperature sampling\n",
        "def generate_text_with_temperature(prompt, temperature=1.0, max_length=100):\n",
        "    # Encode the input prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "    # Generate text with temperature sampling\n",
        "    generated_output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,  # Custom temperature setting\n",
        "        top_k=50,  # Optional: restrict to the top k words\n",
        "        top_p=0.95,  # Optional: nucleus sampling (p-value)\n",
        "        no_repeat_ngram_size=2,  # Optional: prevent repeating n-grams\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the generated output and return as text\n",
        "    generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Test with different temperature values\n",
        "prompt = \"In a faraway land, there existed a mysterious forest where\"\n",
        "\n",
        "temperature_values = [0.7, 1.0, 1.2, 1.5]  # Experiment with different temperatures\n",
        "\n",
        "for temp in temperature_values:\n",
        "    print(f\"\\nGenerated text with temperature = {temp}:\\n\")\n",
        "    generated_text = generate_text_with_temperature(prompt, temperature=temp, max_length=150)\n",
        "    print(generated_text)\n",
        "    print(\"-\" * 50)  # Separator between different temperature results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeEHPGce-klm",
        "outputId": "cdd87286-b117-479c-d4b7-52835c853263"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated text with temperature = 0.7:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a faraway land, there existed a mysterious forest where the sun was shining.\n",
            "\n",
            "\"I'm not sure if it's a forest or not, but it seems like it was there for a long time.\"\n",
            ". . .\n",
            ",\n",
            "-\n",
            "The forest was a place where people lived. It was the place that the people of the world lived in. The people who lived there were the ones who were born in it. They were people that were able to live in the forest. In the past, the humans had lived here, and they had been able, because of their ancestors, to survive in this forest, even if they were killed by the monsters. But now, they are living in a world where they have to\n",
            "--------------------------------------------------\n",
            "\n",
            "Generated text with temperature = 1.0:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a faraway land, there existed a mysterious forest where the sun was shining.\n",
            "\n",
            "\"I'm not sure if it's a forest or not, but it seems like it was there for a long time.\"\n",
            ". . .\n",
            ",\n",
            "-\n",
            "The forest was a place where people lived. It was the place that the people of the world lived in. The people who lived there were the ones who were born in it. They were people that were able to live in the forest. In the past, the humans had lived here, and they had been able, because of their ancestors, to survive in this forest, even if they were killed by the monsters. But now, they are living in a world where they have to\n",
            "--------------------------------------------------\n",
            "\n",
            "Generated text with temperature = 1.2:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a faraway land, there existed a mysterious forest where the sun was shining.\n",
            "\n",
            "\"I'm not sure if it's a forest or not, but it seems like it was there for a long time.\"\n",
            ". . .\n",
            ",\n",
            "-\n",
            "The forest was a place where people lived. It was the place that the people of the world lived in. The people who lived there were the ones who were born in it. They were people that were able to live in the forest. In the past, the humans had lived here, and they had been able, because of their ancestors, to survive in this forest, even if they were killed by the monsters. But now, they are living in a world where they have to\n",
            "--------------------------------------------------\n",
            "\n",
            "Generated text with temperature = 1.5:\n",
            "\n",
            "In a faraway land, there existed a mysterious forest where the sun was shining.\n",
            "\n",
            "\"I'm not sure if it's a forest or not, but it seems like it was there for a long time.\"\n",
            ". . .\n",
            ",\n",
            "-\n",
            "The forest was a place where people lived. It was the place that the people of the world lived in. The people who lived there were the ones who were born in it. They were people that were able to live in the forest. In the past, the humans had lived here, and they had been able, because of their ancestors, to survive in this forest, even if they were killed by the monsters. But now, they are living in a world where they have to\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.11 I Implement a simple LSTM-based text generation model from scratch using Keras and train it on a custom data?\n",
        "#Ans :-\n",
        "!pip install keras numpy tensorflow\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load the text data (replace 'data.txt' with your file path)\n",
        "with open('data.txt', 'r') as file:\n",
        "    text = file.read().lower()\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create sequences of text\n",
        "input_sequences = []\n",
        "next_words = []\n",
        "\n",
        "for i in range(1, len(text) - 50):  # Create sequences of length 50\n",
        "    sequence = text[i:i+50]\n",
        "    input_sequences.append(sequence)\n",
        "    next_words.append(text[i+50])\n",
        "\n",
        "# Convert text sequences to integers\n",
        "X = np.array([tokenizer.texts_to_sequences([seq])[0] for seq in input_sequences])\n",
        "y = np.array([tokenizer.texts_to_sequences([next_word])[0][0] for next_word in next_words])\n",
        "\n",
        "# One-hot encode the output variable y\n",
        "y = to_categorical(y, num_classes=total_words)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding layer (input dimension is total_words, output dimension is 50)\n",
        "model.add(Embedding(total_words, 50, input_length=50))\n",
        "\n",
        "# LSTM layer\n",
        "model.add(LSTM(100, return_sequences=False))\n",
        "\n",
        "# Dense layer for output\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Summarize the model\n",
        "model.summary()\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=50, batch_size=128)\n",
        "# Function to generate text using the trained model\n",
        "def generate_text(model, tokenizer, seed_text, num_words=100):\n",
        "    result = seed_text\n",
        "    for _ in range(num_words):\n",
        "        # Convert the seed text to a sequence of integers\n",
        "        sequence = tokenizer.texts_to_sequences([result])[0]\n",
        "\n",
        "        # Pad the sequence to match the input shape (50)\n",
        "        sequence = np.pad(sequence, (50 - len(sequence), 0), mode='constant')\n",
        "\n",
        "        # Predict the next word\n",
        "        predicted_probs = model.predict(np.array([sequence]))\n",
        "        predicted_word_index = np.argmax(predicted_probs)\n",
        "\n",
        "        # Get the word corresponding to the predicted index\n",
        "        predicted_word = tokenizer.index_word.get(predicted_word_index)\n",
        "\n",
        "        # Append the predicted word to the result\n",
        "        result += ' ' + predicted_word\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example usage\n",
        "seed_text = \"once upon a time\"\n",
        "generated_text = generate_text(model, tokenizer, seed_text, num_words=50)\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hF-T-p_J-2jA",
        "outputId": "a5be5d2f-821d-4638-d3a9-7b6f760a0295"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.12.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.69.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.preprocessing.text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-9dacb88aef68>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install keras numpy tensorflow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.preprocessing.text'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.12  How can you implement text generation using it in a simple custom attention-based architecture?\n",
        "# Ans :-\n",
        "!pip install keras tensorflow numpy\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load the text data (replace 'data.txt' with your file path)\n",
        "with open('data.txt', 'r') as file:\n",
        "    text = file.read().lower()\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create sequences of text\n",
        "input_sequences = []\n",
        "next_words = []\n",
        "\n",
        "for i in range(1, len(text) - 50):  # Create sequences of length 50\n",
        "    sequence = text[i:i+50]\n",
        "    input_sequences.append(sequence)\n",
        "    next_words.append(text[i+50])\n",
        "\n",
        "# Convert text sequences to integers\n",
        "X = np.array([tokenizer.texts_to_sequences([seq])[0] for seq in input_sequences])\n",
        "y = np.array([tokenizer.texts_to_sequences([next_word])[0][0] for next_word in next_words])\n",
        "\n",
        "# One-hot encode the output variable y\n",
        "y = to_categorical(y, num_classes=total_words)\n",
        "from keras import backend as K\n",
        "from keras.layers import Layer\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name='W', shape=(input_shape[2], input_shape[2]), initializer='uniform', trainable=True)\n",
        "        self.b = self.add_weight(name='b', shape=(input_shape[1],), initializer='uniform', trainable=True)\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Score for each timestep\n",
        "        score = K.tanh(K.dot(inputs, self.W) + self.b)\n",
        "        attention_weights = K.softmax(score, axis=1)\n",
        "\n",
        "        # Compute context vector (weighted sum of input values)\n",
        "        context_vector = inputs * K.expand_dims(attention_weights, -1)\n",
        "        context_vector = K.sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding layer (input dimension is total_words, output dimension is 50)\n",
        "model.add(Embedding(total_words, 50, input_length=50))\n",
        "\n",
        "# LSTM layer\n",
        "model.add(LSTM(100, return_sequences=True))  # Return sequences for attention layer\n",
        "\n",
        "# Add the custom attention layer\n",
        "model.add(Attention())\n",
        "\n",
        "# Dense layer for output\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Summarize the model\n",
        "model.summary()\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=50, batch_size=128)\n",
        "# Function to generate text using the trained model\n",
        "def generate_text_with_attention(model, tokenizer, seed_text, num_words=100):\n",
        "    result = seed_text\n",
        "    for _ in range(num_words):\n",
        "        # Convert the seed text to a sequence of integers\n",
        "        sequence = tokenizer.texts_to_sequences([result])[0]\n",
        "\n",
        "        # Pad the sequence to match the input shape (50)\n",
        "        sequence = np.pad(sequence, (50 - len(sequence), 0), mode='constant')\n",
        "\n",
        "        # Predict the next word\n",
        "        predicted_probs = model.predict(np.array([sequence]))\n",
        "        predicted_word_index = np.argmax(predicted_probs)\n",
        "\n",
        "        # Get the word corresponding to the predicted index\n",
        "        predicted_word = tokenizer.index_word.get(predicted_word_index)\n",
        "\n",
        "        # Append the predicted word to the result\n",
        "        result += ' ' + predicted_word\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example usage\n",
        "seed_text = \"once upon a time\"\n",
        "generated_text = generate_text_with_attention(model, tokenizer, seed_text, num_words=50)\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "Tp06BNCF_Shg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O88QQm0__9jv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}