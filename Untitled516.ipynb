{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSS1wi8P2ZVPp7pja/2TAc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nk28-byte/MY-ASSIGNMENT/blob/main/Untitled516.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theroritical Questions"
      ],
      "metadata": {
        "id": "8_a1XM2RSuIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.1 What is the primary goal of Natural Language Processing (NLP)\n",
        "\n",
        "Ans :-You said:\n",
        "What is the primary goal of Natural Language Processing (NLP)\n",
        "ChatGPT said:\n",
        "The primary goal of Natural Language Processing (NLP) is to enable machines to understand, interpret, and generate human language in a way that is meaningful and useful. NLP bridges the gap between human communication and machine understanding, allowing computers to process and analyze large amounts of natural language data.\n",
        "\n",
        "Key objectives of NLP include:\n",
        "\n",
        "Language Understanding: Helping machines comprehend the context, meaning, and intent behind human language (e.g., understanding a question or a command).\n",
        "Language Generation: Allowing machines to generate human-like text or speech (e.g., chatbots, text summarization).\n",
        "Language Translation: Converting text from one language to another (e.g., Google Translate).\n",
        "Information Extraction: Extracting structured information from unstructured text (e.g., identifying names, dates, and events in text).\n",
        "Text Classification: Categorizing text into predefined classes (e.g., spam vs. non-spam messages).\n",
        "Sentiment Analysis: Analyzing the sentiment or emotion expressed in text (e.g., positive, negative, neutral).\n",
        "Speech Processing: Converting speech to text or text to speech.\n",
        "These goals help improve human-computer interaction and enable various applications like virtual assistants, recommendation systems, search engines, and more.\n",
        "\n",
        "\n",
        "\n",
        "Q.2 What does \"tokenization\" refer to in text processing\n",
        "\n",
        "Ans :- Tokenization in text processing refers to the process of breaking down a piece of text (such as a sentence or paragraph) into smaller units called tokens. Tokens can be words, phrases, or even individual characters, depending on the specific application or language being processed.\n",
        "\n",
        "Key Points About Tokenization:\n",
        "Purpose:\n",
        "\n",
        "To convert raw text into a format that is easier for machines to analyze and process.\n",
        "Isentiment analysis, text classification, and machine translation.\n",
        "Types of Tokens:\n",
        "\n",
        "Word Tokens: Splitting text into individual words (e.g., \"This is NLP.\" ‚Üí [\"This\", \"is\", \"NLP\"]).\n",
        "Sentence Tokens: Splitting text into sentences (e.g., \"I love Python. It's fun!\" ‚Üí [\"I love Python.\", \"It's fun!\"]).\n",
        "Character Tokens: Splitting text into individual characters (e.g., \"NLP\" ‚Üí [\"N\", \"L\", \"P\"]).\n",
        "Challenges:\n",
        "\n",
        "Handling Punctuation: Deciding whether punctuation should be treated as separate tokens or attached to words.\n",
        "Compound Words: Dealing with hyphenated words, contractions (e.g., \"it's\" vs. \"it is\"), and languages with no clear word boundaries (e.g., Chinese).\n",
        "Languages and Context: Tokenization rules differ across languages due to unique grammatical and syntactical structures.\n",
        "Applications:\n",
        "\n",
        "Preprocessing: Preparing data for machine learning models.\n",
        "Search Engines: Indexing text for fast retrieval.\n",
        "Text Analysis: Understanding frequency or patterns in language.\n",
        "\n",
        "\n",
        "Q.3 What is the difference between lemmatization and stemming?\n",
        "\n",
        "Ans :- Lemmatization and stemming are both techniques used in Natural Language Processing (NLP) to reduce words to their base or root form. However, they differ in their approach and the type of output they produce.\n",
        "\n",
        "1. Stemming\n",
        "Definition: Stemming reduces a word to its root or stem form by removing prefixes or suffixes, often without considering the word's meaning or grammar.\n",
        "Output: The resulting root form may not be a valid word in the language.\n",
        "Goal: Focuses on computational efficiency, aiming for a quick and simple way to group similar words.\n",
        "Examples:\n",
        "\"running\" ‚Üí \"run\"\n",
        "\"studies\" ‚Üí \"studi\"\n",
        "\"faster\" ‚Üí \"fast\"\n",
        "Algorithm: Uses rules-based approaches, such as Porter Stemmer or Snowball Stemmer.\n",
        "2. Lemmatization\n",
        "Definition: Lemmatization reduces a word to its base or dictionary form (known as the lemma) by considering the context and the word's grammatical structure.\n",
        "Output: The resulting form is a valid word in the language.\n",
        "Goal: Produces more accurate and meaningful results by analyzing the word's part of speech (e.g., noun, verb, adjective).\n",
        "Examples:\n",
        "\"running\" ‚Üí \"run\" (verb)\n",
        "\"studies\" ‚Üí \"study\" (noun)\n",
        "\"faster\" ‚Üí \"fast\" (adjective)\n",
        "Algorithm: Often relies on a vocabulary or a lexicon like WordNet to find the lemma.\n",
        "\n",
        "\n",
        "Q.4 What is the role of regular expressions (regex) in text processing?\n",
        "\n",
        "Ans :- Regular expressions (regex) play a crucial role in text processing by providing a powerful tool for pattern matching and manipulation of text. They are used to search, extract, and modify text based on specific patterns.\n",
        "\n",
        "Key Roles of Regex in Text Processing\n",
        "Searching for Patterns:\n",
        "\n",
        "Find specific words, phrases, or patterns within text.\n",
        "Example: Searching for all email addresses in a document.\n",
        "Regex: \\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\n",
        "Text Extraction:\n",
        "\n",
        "Extract substrings that match a particular pattern.\n",
        "Example: Extract phone numbers from a text.\n",
        "Regex: \\b\\d{3}-\\d{3}-\\d{4}\\b\n",
        "Text Replacement:\n",
        "\n",
        "Replace certain patterns with desired text.\n",
        "Example: Mask credit card numbers for security.\n",
        "Regex: \\b\\d{4}-\\d{4}-\\d{4}-\\d{4}\\b replaced with ****-****-****-****.\n",
        "Validation:\n",
        "\n",
        "Check if a string adheres to a specific format.\n",
        "Example: Validate if a string is a valid URL.\n",
        "Regex: https?://[^\\s]+\n",
        "Splitting Text:\n",
        "\n",
        "Split text into tokens or components based on patterns.\n",
        "Example: Splitting sentences by punctuation marks.\n",
        "Regex: [.!?]\n",
        "Cleaning Text:\n",
        "\n",
        "Remove unwanted characters or patterns from text.\n",
        "Example: Remove extra spaces, special characters, or HTML tags.\n",
        "Regex for extra spaces: \\s+\n",
        "Regex for HTML tags: <[^>]+>\n",
        "\n",
        "\n",
        "Q.5 What is Word2Vec and how does it represent words in a vector space?\n",
        "\n",
        "Ans :- Word2Vec is a popular technique in Natural Language Processing (NLP) for representing words as continuous vector embeddings in a high-dimensional space. It captures the semantic meaning of words by placing similar words close to each other in the vector space.\n",
        "\n",
        "How Word2Vec Works\n",
        "Word2Vec is based on the distributional hypothesis, which states that words occurring in similar contexts tend to have similar meanings. It uses a neural network to learn these word representations (embeddings).\n",
        "\n",
        "Key Models in Word2Vec:\n",
        "CBOW (Continuous Bag of Words):\n",
        "\n",
        "Predicts a target word based on its surrounding context words.\n",
        "Example: For the sentence \"The cat sits on the mat,\" if the context is [\"The\", \"cat\", \"on\", \"the\"], the model predicts \"sits.\"\n",
        "Skip-gram:\n",
        "\n",
        "Predicts context words given a target word.\n",
        "Example: For the sentence \"The cat sits on the mat,\" if the target word is \"sits,\" the model predicts [\"The\", \"cat\", \"on\", \"the\"].\n",
        "\n",
        "Q.6 = How does frequency distribution help in text analysis?\n",
        "\n",
        "Ans :- Frequency distribution is a statistical technique that helps analyze how often each element (e.g., words, phrases, or tokens) occurs in a text. It plays a vital role in text analysis by providing insights into the structure, themes, and focus of the text.\n",
        "\n",
        "Key Benefits of Frequency Distribution in Text Analysis\n",
        "Identifying Important Words:\n",
        "\n",
        "Helps find the most frequently occurring words, which are often indicative of the main topics or themes.\n",
        "Example: In a review dataset, frequent words like \"good\" or \"excellent\" may indicate positive sentiment.\n",
        "Stop-Word Analysis:\n",
        "\n",
        "Reveals the occurrence of common function words (e.g., \"the,\" \"and,\" \"of\"). These can be filtered out to focus on more meaningful content words.\n",
        "Text Summarization:\n",
        "\n",
        "High-frequency terms can be used to create a summary or keywords list for a document.\n",
        "Comparative Analysis:\n",
        "\n",
        "Comparing frequency distributions across different texts or datasets highlights similarities or differences in language usage.\n",
        "Example: Analyzing the vocabulary of Shakespeare‚Äôs plays versus modern novels.\n",
        "Sentiment and Topic Analysis:\n",
        "\n",
        "Frequency of specific words (e.g., \"happy,\" \"sad\") can reveal sentiment.\n",
        "High-frequency domain-specific words (e.g., \"investment,\" \"profit\") point to specific topics.\n",
        "Visualization:\n",
        "\n",
        "Frequency data can be visualized using word clouds, bar charts, or histograms to easily interpret text data patterns.\n",
        "\n",
        "Q.7 Why is text normalization important in NLP?\n",
        "\n",
        "Ans :- Text normalization is a critical step in Natural Language Processing (NLP) because it standardizes text into a consistent format, which helps improve the accuracy and efficiency of downstream NLP tasks. Text data in its raw form often contains noise, inconsistencies, and variations that can hinder analysis and model performance.\n",
        "\n",
        "Key Reasons Why Text Normalization is Important\n",
        "Reducing Variability:\n",
        "\n",
        "Raw text can have multiple representations of the same word or concept (e.g., \"U.S.A.\" vs. \"USA\").\n",
        "Normalization reduces these variations, ensuring that equivalent terms are treated the same.\n",
        "Improving Model Performance:\n",
        "\n",
        "By simplifying text and reducing noise, normalization allows machine learning models to learn more effectively and improves generalization.\n",
        "Handling Noisy Data:\n",
        "\n",
        "Real-world text often includes typos, abbreviations, slang, or inconsistent formatting.\n",
        "Normalization makes the text more structured and reliable for analysis.\n",
        "Facilitating Comparison:\n",
        "\n",
        "Normalized text allows for meaningful comparisons by removing differences that are irrelevant to semantics.\n",
        "Example: Comparing \"it's\" and \"its\" after normalization (removing contractions).\n",
        "\n",
        "\n",
        "Q.8 What is the difference between sentence tokenization and word tokenization?\n",
        "\n",
        "Ans :-Sentence tokenization and word tokenization are both methods of breaking text into smaller components, but they serve different purposes and operate at different levels of granularity.\n",
        "\n",
        "1. Sentence Tokenization\n",
        "Definition:\n",
        "Sentence tokenization splits a given text into individual sentences.\n",
        "\n",
        "Purpose:\n",
        "To process text at the sentence level.\n",
        "Useful for tasks like sentiment analysis, text summarization, or paragraph analysis.\n",
        "Challenges:\n",
        "Dealing with punctuation ambiguities (e.g., \"Dr. Smith is here.\").\n",
        "Handling sentences that end with abbreviations or special cases.\n",
        "Example:\n",
        "Text: \"Hello! How are you doing today? I hope everything is fine.\"\n",
        "\n",
        "Sentence Tokens:\n",
        "\n",
        "[\"Hello!\", \"How are you doing today?\", \"I hope everything is fine.\"]\n",
        "2. Word Tokenization\n",
        "Definition:\n",
        "Word tokenization splits a sentence or text into individual words or tokens.\n",
        "\n",
        "Purpose:\n",
        "To process text at the word level for tasks like word embeddings, text classification, and machine translation.\n",
        "Helps analyze the frequency and importance of words.\n",
        "Challenges:\n",
        "Handling punctuation, contractions, and special characters.\n",
        "Determining how to treat compound words (e.g., \"self-driving\" vs. \"self\" and \"driving\").\n",
        "Example:\n",
        "Text: \"How are you doing today?\"\n",
        "\n",
        "Word Tokens:\n",
        "\n",
        "[\"How\", \"are\", \"you\", \"doing\", \"today\", \"?\"]\n",
        "\n",
        "\n",
        "Q.9  What are co-occurrence vectors in NLP?\n",
        "\n",
        "Ans :- Co-occurrence vectors in NLP are a type of word representation that capture the relationship between words based on their occurrence together in a given context. They are created using co-occurrence matrices, which count how often words appear together in a predefined window or document.\n",
        "\n",
        "Key Concepts Behind Co-occurrence Vectors\n",
        "Co-occurrence Matrix:\n",
        "\n",
        "A matrix where rows and columns represent words in the vocabulary.\n",
        "Each cell\n",
        "(\n",
        "ùëñ\n",
        ",\n",
        "ùëó\n",
        ")\n",
        "(i,j) contains the count of how often word\n",
        "ùëó\n",
        "j occurs in the context of word\n",
        "ùëñ\n",
        "i.\n",
        "\n",
        "\n",
        "Q.10 What is the significance of lemmatization in improving NLP tasks?\n",
        "\n",
        "Ans :- Lemmatization plays a significant role in improving Natural Language Processing (NLP) tasks by reducing words to their base or dictionary form, known as the lemma. Unlike stemming, which merely chops off word endings, lemmatization ensures that the result is a valid word in the language, preserving its meaning. This process contributes to cleaner, more effective data representation for a variety of NLP tasks.\n",
        "\n",
        "Key Significance of Lemmatization in Improving NLP Tasks\n",
        "Reduces Redundancy:\n",
        "\n",
        "Lemmatization helps in reducing the number of unique word forms. For example, \"running,\" \"ran,\" and \"runs\" are all reduced to the lemma \"run.\" This means that words with the same meaning are treated as one, reducing data sparsity.\n",
        "Example: \"Better\" ‚Üí \"good\", \"mice\" ‚Üí \"mouse\"\n",
        "Improves Semantic Understanding:\n",
        "\n",
        "By converting words to their base form, lemmatization ensures that words with the same root are treated similarly in tasks like sentiment analysis, text classification, or topic modeling.\n",
        "Example: Both \"amazing\" and \"amazement\" are reduced to \"amaze\", preserving their core meaning.\n",
        "Enhances Text Normalization:\n",
        "\n",
        "Lemmatization is often part of the text preprocessing pipeline, allowing for more accurate results in downstream NLP tasks like machine learning and information retrieval. It standardizes variations of words and removes noise.\n",
        "Example: In a search engine, querying \"running\" should yield the same results as \"run.\"\n",
        "Improves NLP Model Performance:\n",
        "\n",
        "By reducing words to their root forms, lemmatization reduces the complexity of the dataset, enabling NLP models to generalize better and focus on the meaning rather than specific word forms. This leads to improved accuracy in tasks like classification, named entity recognition (NER), and machine translation.\n",
        "\n",
        "\n",
        "Q.11 What is the primary use of word embeddings in NLP?\n",
        "\n",
        "Ans :- Word embeddings are a fundamental technique in Natural Language Processing (NLP) that represents words in a continuous vector space, where semantically similar words are mapped to nearby points. The primary use of word embeddings is to improve the way machines understand and process human language by capturing semantic meaning and relationships between words.\n",
        "\n",
        "Here are the primary uses of word embeddings in NLP:\n",
        "\n",
        "1. Capturing Semantic Relationships\n",
        "Word embeddings allow words with similar meanings to have similar vector representations, which helps models understand synonyms (e.g., \"king\" and \"queen\") and analogies (e.g., \"man\" is to \"woman\" as \"king\" is to \"queen\"). This is in contrast to traditional one-hot encoding, where words are represented as sparse vectors with no notion of similarity.\n",
        "\n",
        "Example: \"cat\" and \"dog\" would be closer in the vector space than \"cat\" and \"car\" because both are animals.\n",
        "2. Enhancing Machine Learning Models\n",
        "Word embeddings serve as a compact and dense representation of words, making them more efficient for machine learning algorithms. By representing words as vectors, they can be input directly into machine learning models (e.g., neural networks, transformers) for tasks like sentiment analysis, text classification, and translation.\n",
        "\n",
        "Example: In sentiment analysis, embeddings help the model better understand that \"happy\" and \"joyful\" are related, while \"sad\" and \"angry\" might be opposites.\n",
        "\n",
        "\n",
        "Q.12 What is the primary use of word embeddings in NLP?\n",
        "\n",
        "Ans :- The primary use of word embeddings in NLP is to represent words in a continuous, high-dimensional vector space where words with similar meanings are mapped to nearby points. This representation captures semantic relationships between words, allowing machine learning models to better understand context and meaning.\n",
        "\n",
        "Here are some key primary uses of word embeddings in NLP:\n",
        "\n",
        "1. Capturing Semantic Meaning\n",
        "Word embeddings enable machines to capture the semantic relationships between words. Words with similar meanings (e.g., \"king\" and \"queen,\" \"cat\" and \"dog\") are represented by similar vectors, which helps improve tasks like text classification, sentiment analysis, and question answering.\n",
        "2. Improving Model Performance\n",
        "Word embeddings provide a compact and efficient representation of words, making them easier for machine learning models to process compared to traditional methods like one-hot encoding. This leads to better performance in tasks such as named entity recognition (NER), part-of-speech tagging, and text generation.\n",
        "\n",
        "\n",
        "Q.13 What is an annotator in NLP?\n",
        "\n",
        "Ans :- In Natural Language Processing (NLP), an annotator refers to a tool, system, or process that adds labels or annotations to a given text. These annotations provide additional information about the text, such as part-of-speech tags, named entities, sentiment, syntactic structures, or other linguistic features. Annotators are typically used during the preprocessing phase or manual data labeling to prepare datasets for training or evaluating NLP models.\n",
        "\n",
        "Types of Annotations in NLP\n",
        "Part-of-Speech Tagging (POS):\n",
        "\n",
        "Annotators can label each word in a sentence with its part of speech (e.g., noun, verb, adjective).\n",
        "Example: \"The cat sleeps.\" ‚Üí [The (Determiner), cat (Noun), sleeps (Verb)]\n",
        "Named Entity Recognition (NER):\n",
        "\n",
        "Annotators identify and label named entities in the text, such as names of people, organizations, locations, dates, etc.\n",
        "Example: \"Apple Inc. was founded by Steve Jobs.\" ‚Üí [Apple Inc. (Organization), Steve Jobs (Person)]\n",
        "Sentiment Annotation:\n",
        "\n",
        "Annotators label text with sentiment categories (positive, negative, neutral) to assist with tasks like sentiment analysis.\n",
        "Example: \"I love this product!\" ‚Üí Positive\n",
        "\n",
        "\n",
        "Q.14 What are the key steps in text processing before applying machine learning models?\n",
        "\n",
        "Ans :-Before applying machine learning (ML) models to text data, several key text processing steps are performed to transform raw text into a format that is suitable for machine learning. These steps ensure that the data is clean, structured, and ready for analysis. Here are the essential steps involved in text processing:\n",
        "\n",
        "1. Text Cleaning\n",
        "Remove Noise: This step involves eliminating unnecessary elements in the text that might interfere with processing, such as:\n",
        "Punctuation marks\n",
        "Special characters\n",
        "Numbers (if irrelevant to the analysis)\n",
        "HTML tags (if scraping from websites)\n",
        "Extra spaces and tabs\n",
        "Example: The text \"Hello!! This is an <b>example</b> text #123.\" would be cleaned to \"Hello This is an example text\".\n",
        "2. Tokenization\n",
        "Tokenization refers to splitting text into smaller units (tokens), usually words or sentences, to enable more manageable processing.\n",
        "Word Tokenization: Breaking text into individual words.\n",
        "Sentence Tokenization: Splitting a text into its individual sentences.\n",
        "Example: The sentence \"I love machine learning!\" would be tokenized into [\"I\", \"love\", \"machine\", \"learning\"].\n",
        "3. Lowercasing\n",
        "Converting all text to lowercase ensures that words like \"Machine\" and \"machine\" are treated as the same word.\n",
        "Example: \"I Love Python\" ‚Üí \"i love python\"\n",
        "\n",
        "\n",
        "Q.15 What is the history of NLP and how has it evolved?\n",
        "\n",
        "Ans :- The history of Natural Language Processing (NLP) is rich and spans several decades of research, technological advancements, and evolving methodologies. NLP has developed from basic rule-based systems to sophisticated AI-powered techniques that can understand, generate, and process human language. Here‚Äôs an overview of the evolution of NLP:\n",
        "\n",
        "1. Early Foundations (1950s - 1970s)\n",
        "1950s: The birth of computational linguistics can be traced back to the work of Alan Turing. In his 1950 paper \"Computing Machinery and Intelligence,\" Turing introduced the famous Turing Test to assess a machine's ability to exhibit intelligent behavior equivalent to a human.\n",
        "\n",
        "First NLP Systems: Early attempts focused on machine translation, with systems designed to translate text between languages. The Georgetown-IBM experiment in 1954 was one of the first successful demonstrations of machine translation, translating Russian sentences into English.\n",
        "1960s - 1970s:\n",
        "\n",
        "The focus shifted to rule-based systems where linguists manually crafted sets of rules for parsing, sentence structure, and word interpretation.\n",
        "ELIZA (1966): A program developed by Joseph Weizenbaum that simulated a therapist's conversation with a patient, illustrating an early form of natural language interaction.\n",
        "Early Linguistic Theories: In the 1970s, syntactic parsing based on transformational grammar (e.g., Noam Chomsky‚Äôs theories) was used to build systems that could analyze sentence structures.\n",
        "\n",
        "\n",
        "Q.16 Why is sentence processing important in NLP?\n",
        "\n",
        "Ans :- Sentence processing is crucial in Natural Language Processing (N NLP) because it enables machines to understand and interpret the structure, meaning, and relationships within text at a higher level than word-based analysis. Here‚Äôs why sentence processing is essential:\n",
        "\n",
        "1. Understanding Sentence Structure\n",
        "Syntax and Grammar: Sentences are built on grammatical rules and syntactic structures. By processing sentences, NLP models can analyze sentence structure and the relationship between words (subject, object, verb, etc.), which is essential for tasks like parsing and part-of-speech tagging.\n",
        "Example: In the sentence \"The cat chased the mouse,\" sentence processing helps the system understand that \"The cat\" is the subject, \"chased\" is the verb, and \"the mouse\" is the object.\n",
        "2. Extracting Meaning from Context\n",
        "Contextual Understanding: Sentences often provide context for words that helps determine their meaning. Processing sentences helps resolve ambiguities that arise in word-level analysis. For example, the word \"bank\" can mean a financial institution or the side of a river, but sentence-level processing provides context to disambiguate this.\n",
        "Example: \"He went to the bank to deposit money.\" (Financial institution) vs. \"He sat by the bank of the river.\" (Side of the river)\n",
        "\n",
        "\n",
        "Q.17  How do word embeddings improve the understanding of language semantics in NLP?\n",
        "\n",
        "Ans :- Word embeddings are a type of representation used in Natural Language Processing (NLP) that map words or phrases to high-dimensional vectors in a continuous vector space. These embeddings are designed in such a way that semantically similar words are positioned close to each other in the vector space. Word embeddings significantly improve the understanding of language semantics in several ways:\n",
        "\n",
        "1. Capturing Semantic Similarity\n",
        "Word embeddings allow machines to understand the semantic relationships between words. Words that have similar meanings (e.g., \"king\" and \"queen\") are represented by vectors that are geometrically close to each other in the vector space.\n",
        "Example: In Word2Vec or GloVe, words like \"dog\" and \"puppy\" or \"happy\" and \"joyful\" will have vectors close together because they share similar meanings or are often used in similar contexts.\n",
        "\n",
        "\n",
        "Q.18 How does the frequency distribution of words help in text classification?\n",
        "\n",
        "Ans :- The frequency distribution of words plays a significant role in text classification because it helps models understand the importance and relevance of words in differentiating between different categories or classes. By analyzing the frequency with which words appear in a document or corpus, a model can gain insights into the content of the text, which can be used for tasks such as spam detection, sentiment analysis, or topic classification. Here's how the frequency distribution contributes to text classification:\n",
        "\n",
        "1. Identifying Key Features (Important Words)\n",
        "The frequency of words helps in identifying which words are important for differentiating between classes. Words that occur frequently within a certain category are likely to be strong indicators of that class.\n",
        "Example: In sentiment analysis, words like \"good\", \"happy\", \"love\" may appear frequently in positive reviews, while words like \"bad\", \"hate\", \"poor\" are more frequent in negative reviews. By examining the frequency distribution, the classifier can learn which words are associated with specific sentiments.\n",
        "\n",
        "\n",
        "Q.19 What are the advantages of using regex in text cleaning?\n",
        "\n",
        "Ans :-Using regular expressions (regex) in text cleaning offers several advantages in text preprocessing, especially when dealing with unstructured data. Here are some of the key advantages:\n",
        "\n",
        "1. Flexibility in Pattern Matching\n",
        "Regex provides a flexible and powerful way to match complex text patterns. It allows you to define patterns for specific character sequences, words, or structures that you want to extract or remove from the text.\n",
        "Example: You can use regex to identify and remove phone numbers, email addresses, URLs, or any other structured data from a text corpus.\n",
        "2. Efficiency in Handling Large Text Data\n",
        "Regex can be very efficient when processing large volumes of text. Instead of manually parsing through the text or using multiple conditions, regex can handle large datasets with a single line of code to find, replace, or modify text patterns.\n",
        "Example: For cleaning social media posts, regex can quickly remove unwanted special characters or filter hashtags without processing each post line by line.\n",
        "3. Customization for Specific Use Cases\n",
        "With regex, you can tailor your text cleaning process to suit specific needs or use cases. You can create custom patterns for a variety of scenarios like handling misspelled words, formatting issues, or identifying and cleaning specific types of data.\n",
        "\n",
        "\n",
        "Q.20 What is the difference between word2vec and doc2vec?\n",
        "\n",
        "Ans :- Both Word2Vec and Doc2Vec are popular techniques in Natural Language Processing (NLP) used to represent text data as dense vectors, but they differ in the scope of text they represent and the way they are trained. Here's the key difference between the two:\n",
        "\n",
        "1. Representation Scope\n",
        "Word2Vec:\n",
        "Word2Vec represents individual words as dense vectors. It is designed to learn vector representations of words based on their context in a corpus of text.\n",
        "It focuses on learning the semantic meaning of words and how they relate to one another based on their usage in sentences.\n",
        "Example: The word \"king\" might be represented by a vector in a multi-dimensional space, and words like \"queen\", \"prince\", or \"royalty\" would have similar vector representations due to their semantic relationships.\n",
        "Doc2Vec:\n",
        "Doc2Vec (also known as Paragraph2Vec) extends Word2Vec to represent entire documents or paragraphs as vectors. It generates a vector for a larger text unit, such as a sentence, paragraph, or entire document, in addition to word vectors.\n",
        "It aims to capture the semantic meaning of the entire document, including its context and the relationships between words in that document.\n",
        "Example: A paragraph or a document about \"machine learning\" would be represented by a single vector that captures the overall theme of the text, while still considering individual word meanings.\n",
        "\n",
        "\n",
        "Q.21 Why is understanding text normalization important in NLP?\n",
        "\n",
        "Ans :- Understanding text normalization is crucial in Natural Language Processing (NLP) because it plays a key role in preparing text data for further analysis or machine learning tasks. Text normalization is the process of transforming text into a standard format to reduce the complexity and inconsistency inherent in raw text. Here's why text normalization is important:\n",
        "\n",
        "1. Reducing Variability in Text\n",
        "Text data is often noisy and inconsistent: Raw text can contain a wide variety of formats, such as different spellings, abbreviations, or punctuation marks. Normalizing the text helps reduce this variability by transforming the text into a standard form, making it easier for models to process.\n",
        "Example: The words \"color\" and \"colour\" (British vs. American English) can be normalized to a common form (\"color\") so the model treats them as equivalent.\n",
        "2. Improving Model Performance\n",
        "Consistency in data: By normalizing the text, you ensure that the model encounters consistent and predictable patterns in the data, leading to improved performance in NLP tasks such as text classification, sentiment analysis, or named entity recognition (NER).\n",
        "Example: A model will perform better in sentiment analysis if all instances of \"not good\" are converted to a uniform format, rather than having variations like \"not-so-good\", \"not good\", or \"not really good\".\n",
        "3. Reducing Dimensionality\n",
        "Text normalization often involves reducing the size of the vocabulary: For example, transforming all text to lowercase or removing stop words helps ensure that the model doesn't have to handle unnecessary variations, thus reducing the dimensionality of the feature space.\n",
        "Example: \"Apple\", \"apple\", and \"APPLE\" can all be converted to \"apple\" to reduce the feature space and avoid treating them as different words.\n",
        "\n",
        "Q.22 = How does word count help in text analysis?\n",
        "\n",
        "Ans :- Word count is a fundamental metric used in text analysis that helps provide basic insights into the content of a document or corpus. It serves as a simple but effective measure for understanding various aspects of the text, and it plays an important role in many NLP tasks. Here‚Äôs how word count helps in text analysis:\n",
        "\n",
        "1. Understanding Text Length and Complexity\n",
        "Word count helps to determine the length of a text, which can provide insights into its complexity. Longer texts may be more comprehensive, while shorter texts may focus on specific information.\n",
        "Example: A lengthy blog post may indicate a detailed explanation, whereas a short tweet might be more concise and to the point.\n",
        "Use: In sentiment analysis or classification tasks, longer texts may have more context and richer information, which could influence how the text is processed or classified.\n",
        "2. Feature Extraction for Machine Learning Models\n",
        "Word count can serve as an important feature in machine learning models for text classification or regression tasks. The number of words can correlate with other features like sentiment or topic, or it may serve as a simple heuristic to distinguish between categories.\n",
        "Example: In a spam detection system, the word count might help identify unusually long or short messages that are more likely to be spam.\n",
        "Use: Word count can be combined with other features such as the frequency of certain keywords to build a more accurate model.\n",
        "\n",
        "\n",
        "Q.23 How does lemmatization help in NLP tasks like search engines and chatbots?\n",
        "\n",
        "Ans :- Lemmatization plays a crucial role in improving the effectiveness of NLP tasks like search engines and chatbots by transforming words into their base or dictionary forms (lemmas). This process reduces the complexity of language and ensures that words with similar meanings are treated consistently. Here's how lemmatization helps in these tasks:\n",
        "\n",
        "1. Improved Search Engine Results\n",
        "In search engines, lemmatization helps improve information retrieval by ensuring that different forms of a word are treated as the same word. This leads to more accurate search results and a better user experience.\n",
        "\n",
        "Handling Word Variations:\n",
        "\n",
        "Words like \"running\", \"ran\", and \"runs\" are all variations of the lemma \"run\". If a user searches for \"run\", the search engine can return relevant results that include all forms of the word.\n",
        "Without lemmatization, a search query for \"run\" might not match documents containing \"running\" or \"ran\", leading to less relevant results.\n",
        "Enhancing Relevance:\n",
        "\n",
        "Lemmatization ensures that search engines are able to understand the underlying intent of a search query. For example, a search for \"better\" could be reduced to the lemma \"good\", which helps the engine fetch results more closely related to the user's intent.\n",
        "Efficient Indexing:\n",
        "\n",
        "By reducing words to their base forms, lemmatization reduces the number of unique terms that need to be indexed, helping optimize search engine efficiency and relevance in retrieving results.\n",
        "\n",
        "\n",
        "Q.24 What is the purpose of using Doc2Vec in text processing?\n",
        "\n",
        "Ans :- Doc2Vec (Document to Vector) is an extension of the Word2Vec model, designed to represent entire documents as vectors (dense numerical representations). Its primary purpose is to capture the semantic meaning of an entire document or paragraph, rather than just individual words, making it especially useful in tasks where the context and overall meaning of a document are more important than individual word meanings. Here‚Äôs how Doc2Vec serves various purposes in text processing:\n",
        "\n",
        "\n",
        "Q.25 What is the importance of sentence processing in NLP?\n",
        "\n",
        "Ans :- Sentence processing in Natural Language Processing (NLP) is a critical component that focuses on analyzing, understanding, and generating meaningful sentences from raw text. It plays a crucial role in enabling machines to comprehend language at the sentence level, which is essential for many NLP applications. Here's why sentence processing is important in NLP:\n",
        "\n",
        "1. Understanding Sentence Structure (Syntax)\n",
        "Sentence processing allows machines to understand the structure of a sentence, including the relationships between words and phrases. This is essential for comprehending the grammatical organization of sentences.\n",
        "\n",
        "Parsing: Sentence processing often involves syntactic parsing, where the sentence is broken down into its grammatical components (such as subjects, verbs, and objects). This helps machines to grasp the rules of grammar and how words combine to form meaningful statements.\n",
        "Example: In the sentence \"The cat sat on the mat,\" sentence processing helps identify \"The cat\" as the subject, \"sat\" as the verb, and \"on the mat\" as a prepositional phrase.\n",
        "\n",
        "\n",
        "Q.26 What is text normalization, and what are the common techniques used in it?\n",
        "\n",
        "Ans :- Text normalization is the process of transforming text into a consistent and standard format to simplify its analysis in Natural Language Processing (NLP) tasks. The goal is to reduce variability in the text by addressing issues such as case differences, punctuation, special characters, and word forms. This helps improve the accuracy and efficiency of NLP models by ensuring that text is processed in a uniform way.\n",
        "\n",
        "Here are some of the common techniques used in text normalization:\n",
        "\n",
        "1. Lowercasing\n",
        "Converting all characters in the text to lowercase helps eliminate case differences between words, ensuring that words like \"Apple\" and \"apple\" are treated as the same word.\n",
        "\n",
        "Example:\n",
        "\"The Quick Fox\" ‚Üí \"the quick fox\"\n",
        "2. Removing Punctuation\n",
        "Punctuation marks (such as periods, commas, exclamation marks, etc.) are often irrelevant to the analysis of text and can be removed to avoid introducing unnecessary noise into the text.\n",
        "\n",
        "Example:\n",
        "\"Hello, world!\" ‚Üí \"Hello world\"\n",
        "\n",
        "\n",
        "Q.27 Why is word tokenization important in NLP?\n",
        "\n",
        "Ans :- Word tokenization is a crucial step in Natural Language Processing (NLP), as it breaks down a piece of text into individual words or tokens. This process plays a significant role in enabling machines to understand and process language at the word level, allowing for a variety of NLP tasks. Here‚Äôs why word tokenization is important:\n",
        "\n",
        "1. Preprocessing for Further NLP Tasks\n",
        "Tokenization serves as the foundation for most NLP tasks. Once text is tokenized, it can be processed further for tasks like text classification, sentiment analysis, and named entity recognition (NER).\n",
        "\n",
        "Example: Tokenizing a sentence like \"I love programming!\" into [\"I\", \"love\", \"programming\"] allows each word to be analyzed independently, enabling accurate sentiment classification (positive sentiment in this case).\n",
        "2. Simplifying Text Representation\n",
        "By breaking down text into smaller units (words), tokenization simplifies the representation of text, making it easier for algorithms to process. It helps convert raw text into a structured form that can be fed into machine learning models.\n",
        "\n",
        "Example: The sentence \"Natural Language Processing is awesome!\" becomes the tokenized form [\"Natural\", \"Language\", \"Processing\", \"is\", \"awesome\"]. This allows models to work with individual words rather than complex sentences.\n",
        "3. Handling Variations in Word Forms\n",
        "Tokenization helps to identify different forms of a word that can appear in text. This is crucial for tasks like lemmatization or stemming, where the goal is to reduce words to their base forms.\n",
        "\n",
        "Example: Tokenization breaks down \"running\" and \"ran\" into separate tokens, making it possible to later normalize both forms to their root form (\"run\").\n",
        "\n",
        "Q.28 How does sentence tokenization differ from word tokenization in NLP?\n",
        "\n",
        "Ans :- Sentence tokenization and word tokenization are both essential techniques in Natural Language Processing (NLP), but they differ in terms of the level of granularity at which they operate and the specific tasks they are used for. Here‚Äôs a breakdown of the key differences between the two:\n",
        "\n",
        "1. Granularity\n",
        "Sentence Tokenization: This involves dividing a text into sentences. The goal is to separate the text into coherent, syntactic units that correspond to individual sentences.\n",
        "\n",
        "Example:\n",
        "Text: \"I love programming. It‚Äôs my favorite hobby.\"\n",
        "Sentence Tokenization: [\"I love programming.\", \"It‚Äôs my favorite hobby.\"]\n",
        "Word Tokenization: This involves dividing the text into individual words or tokens. The goal is to split the text into the smallest units of meaningful content, typically separated by spaces or punctuation marks.\n",
        "\n",
        "Example:\n",
        "Text: \"I love programming.\"\n",
        "Word Tokenization: [\"I\", \"love\", \"programming\"]\n",
        "2. Purpose and Usage\n",
        "\n",
        "Q.29 What is the primary purpose of text processing in NLP?\n",
        "\n",
        "Ans :- The primary purpose of text processing in Natural Language Processing (NLP) is to transform raw text into a format that can be efficiently and accurately analyzed by computational models. Text processing is essential for preparing data so that machines can understand and interpret human language, enabling a wide range of NLP tasks. Here‚Äôs a breakdown of the key purposes of text processing:\n",
        "\n",
        "1. Data Cleaning and Standardization\n",
        "Text data is often unstructured, noisy, and inconsistent. Text processing helps clean and standardize the data by addressing issues such as inconsistent formatting, typos, irrelevant information, and noise (e.g., punctuation, special characters, extra spaces).\n",
        "\n",
        "Example: Removing irrelevant characters or standardizing word forms to ensure consistency (e.g., \"running\" ‚Üí \"run\").\n",
        "2. Tokenization\n",
        "Tokenization is the process of breaking text into smaller units like words, sentences, or subwords, which allows the machine to handle individual components of the text. These tokens serve as the building blocks for many NLP tasks, such as text classification, sentiment analysis, or named entity recognition (NER).\n",
        "\n",
        "Example: \"I love NLP!\" ‚Üí [\"I\", \"love\", \"NLP\"]\n",
        "3. Feature Extraction\n",
        "Text processing helps convert text into meaningful features that machine learning algorithms can use. It extracts useful information from the raw text, such as word counts, term frequencies, or embeddings. This is essential for tasks like text classification, topic modeling, and information retrieval.\n",
        "\n",
        "Example: Transforming raw text into a Bag-of-Words representation or using word embeddings (e.g., Word2Vec).\n",
        "\n",
        "\n",
        "Q.30 What are the key challenges in NLP?\n",
        "\n",
        "Ans :- Natural Language Processing (NLP) faces several challenges, which include:\n",
        "\n",
        "Ambiguity:\n",
        "\n",
        "Lexical Ambiguity: Words having multiple meanings depending on context (e.g., \"bank\" can mean a financial institution or the side of a river).\n",
        "Syntactic Ambiguity: Sentences with multiple grammatical structures (e.g., \"I saw the man with the telescope\").\n",
        "Semantic Ambiguity: When the meaning of a phrase is unclear (e.g., \"She cannot bear children\" could mean she can‚Äôt tolerate children or she is unable to have children).\n",
        "Context Understanding:\n",
        "\n",
        "Grasping the broader context of words and sentences is difficult for NLP models. Understanding the subject, mood, and intent behind a statement or question is often a complex task.\n",
        "Word Sense Disambiguation:\n",
        "\n",
        "The challenge of determining the correct meaning of a word based on context. For example, \"bark\" can refer to the sound a dog makes or the outer covering of a tree.\n",
        "Sarcasm and Irony:\n",
        "\n",
        "Detecting sarcasm and irony is difficult because the literal meaning of words can differ greatly from the intended meaning.\n",
        "Multilingual and Cross-lingual Issues:\n",
        "\n",
        "Different languages have unique structures, idioms, and rules, making it challenging for NLP models to work across various languages.\n",
        "Translating between languages or understanding context in a multilingual environment can introduce errors.\n",
        "Handling Diverse Vocabulary:\n",
        "\n",
        "Slang, abbreviations, and informal language (especially on social media) can make text harder for NLP models to process accurately.\n",
        "Named Entity Recognition (NER):\n",
        "\n",
        "Extracting and categorizing entities (like names, locations, dates) is tricky because entities can vary widely and contextually.\n",
        "Text Summarization:\n",
        "\n",
        "Generating concise summaries while retaining important information is still a major challenge, especially for long documents.\n",
        "Data Quality and Preprocessing:\n",
        "\n",
        "Data noise (e.g., misspellings, ungrammatical sentences, or irrelevant information) makes it harder for models to learn accurate representations of language.\n",
        "Bias and Fairness:\n",
        "\n",
        "NLP models often inherit biases present in training data, leading to discriminatory outputs, such as gender or racial bias in text classification.\n",
        "\n",
        "\n",
        "Q.31 How do co-occurrence vectors represent relationships between words?\n",
        "\n",
        "Ans :- Co-occurrence vectors represent relationships between words by capturing how frequently and in what contexts words appear together in a given corpus of text. The core idea is that words that appear in similar contexts tend to have similar meanings or relationships.\n",
        "\n",
        "Here‚Äôs how co-occurrence vectors represent word relationships:\n",
        "\n",
        "Contextual Relationship:\n",
        "\n",
        "Words that co-occur frequently in similar contexts are considered to be related. For example, in a sentence like \"The cat sat on the mat,\" the words \"cat\" and \"mat\" are likely to appear in similar contexts, establishing a relationship between them.\n",
        "Creating Co-occurrence Matrices:\n",
        "\n",
        "A co-occurrence matrix is constructed where each row and column corresponds to a word in the vocabulary. The values in the matrix represent how often a word (from the row) appears in the context of another word (from the column).\n",
        "Typically, this context is defined as words within a certain window around the target word. For instance, a window size of 3 would consider the three words before and after the target word in a sentence.\n",
        "Matrix Example:\n",
        "\n",
        "Suppose you have the following small corpus: \"The cat sat on the mat.\"\n",
        "\n",
        "You can create a co-occurrence matrix like this (ignoring stop words for simplicity):\n",
        "\n",
        "the\tcat\tsat\ton\tmat\n",
        "the\t2\t1\t1\t1\t1\n",
        "cat\t1\t0\t1\t1\t1\n",
        "sat\t1\t1\t0\t1\t1\n",
        "on\t1\t1\t1\t0\t1\n",
        "mat\t1\t1\t1\t1\t0\n",
        "The value \"1\" indicates that the word from the row appears in the context of the word from the column within a defined window, and the value \"0\" means no co-occurrence.\n",
        "Word Similarity:\n",
        "\n",
        "By looking at the co-occurrence vectors (or rows) of two words, you can determine their similarity. For instance, the row for \"cat\" and \"mat\" would be quite similar because they frequently appear together in similar contexts. This similarity can be calculated using metrics such as cosine similarity.\n",
        "Semantic Relationships:\n",
        "\n",
        "Co-occurrence vectors can capture a variety of semantic relationships, such as synonyms (words that appear in similar contexts, e.g., \"cat\" and \"dog\"), antonyms (words that rarely co-occur, e.g., \"hot\" and \"cold\"), and thematic relationships (words related to specific topics, like \"doctor\" and \"medicine\").\n",
        "Limitations:\n",
        "\n",
        "Sparsity: The co-occurrence matrix can be very large, and many word pairs will not co-occur enough to provide meaningful data.\n",
        "Context Size: The window size (how many surrounding words are considered) can impact the quality of relationships captured. A small window might miss long-range dependencies, while a large window may dilute context.\n",
        "Polysemy: Words with multiple meanings (e.g., \"bank\" as a financial institution or the side of a river) may result in mixed co-occurrence patterns.\n",
        "In summary, co-occurrence vectors provide a way to quantify relationships between words by measuring how often they appear together in similar contexts. These vectors are fundamental for creating word embeddings like Word2Vec, which leverage these relationships to represent words in a dense vector space for downstream NLP tasks.\n",
        "\n",
        "\n",
        "Q.32 What is the role of frequency distribution in text analysis?\n",
        "\n",
        "Ans :- In text analysis, frequency distribution plays a critical role in identifying and quantifying the occurrences of various elements in a text, such as words, characters, or n-grams (combinations of words). By analyzing the frequency distribution, we can extract valuable insights about the content, structure, and underlying patterns in the text. Here's how it contributes to text analysis:\n",
        "\n",
        "1. Understanding Word Importance:\n",
        "Word Frequency: Frequency distributions help identify the most frequently occurring words in a text or corpus. Words that occur frequently often carry key themes, topics, or subjects relevant to the content. For example, in a news article about climate change, words like \"environment,\" \"global,\" \"warming,\" and \"carbon\" may appear frequently, indicating the central topic.\n",
        "\n",
        "Stop Words Removal: Commonly used words like \"the,\" \"and,\" \"is,\" and \"in\" (known as stop words) typically occur very frequently. Frequency distribution helps to identify and remove these from analysis to focus on more meaningful terms.\n",
        "\n",
        "2. Feature Extraction for Machine Learning:\n",
        "In tasks like text classification or sentiment analysis, the frequency distribution of words serves as a feature for machine learning models. For instance, the number of times certain words appear can be used to predict categories or sentiment. A word's frequency (often represented as a vector) is input into the model to make predictions.\n",
        "3. Topic Modeling and Text Classification:\n",
        "Topic Modeling: The frequency of specific words can indicate the presence of certain topics within a document. By analyzing the distribution of words across multiple documents, models like Latent Dirichlet Allocation (LDA) can uncover hidden topics in large datasets.\n",
        "\n",
        "Text Classification: Frequency distributions are often used to help classify texts based on the commonality of certain words. For example, if a document contains frequent words associated with finance, the model might classify it as a financial document.\n",
        "\n",
        "4. Identifying Key Phrases and Patterns:\n",
        "High-frequency word combinations (like \"climate change\" or \"social media\") can be identified and analyzed to understand recurring themes or concepts. In some cases, n-grams (combinations of words) are analyzed for this purpose, helping to recognize important phrases and their significance in the text.\n",
        "\n",
        "\n",
        "Q.33 What is the impact of word embeddings on NLP tasks?\n",
        "\n",
        "Ans :- Word embeddings have had a profound impact on various NLP tasks by providing a way to represent words in a continuous vector space where semantically similar words are mapped closer together. Here's how they have influenced and improved NLP:\n",
        "\n",
        "1. Capturing Semantic Relationships:\n",
        "Contextual Similarity: Traditional one-hot encoding or bag-of-words models treated words as discrete entities with no concept of similarity. Word embeddings, such as Word2Vec, GloVe, and fastText, represent words as dense vectors in a continuous space, allowing semantically similar words to be closer in this space. For example, \"king\" and \"queen\" would have vectors that are closer to each other than \"king\" and \"dog.\"\n",
        "\n",
        "Word Analogies: Embeddings capture rich relationships like analogies. For instance, using vector arithmetic, you can compute analogies such as:\n",
        "\n",
        "\"king\" - \"man\" + \"woman\" = \"queen\" This ability to capture analogical relationships has been invaluable in various tasks, such as language understanding and machine translation.\n",
        "2. Improved Text Classification:\n",
        "Word embeddings enhance text classification tasks by transforming words into numerical representations that preserve semantic meanings. Unlike traditional bag-of-words approaches, embeddings provide more context-sensitive representations, which improve the performance of models in tasks like sentiment analysis, spam detection, and topic classification.\n",
        "Embeddings allow models to recognize the contextual meaning of words, such as understanding that \"bank\" in the context of finance is different from \"bank\" in the context of a river.\n",
        "\n",
        "\n",
        "Q.34  What is the purpose of using lemmatization in text preprocessing?\n",
        "\n",
        "Ans :-The purpose of lemmatization in text preprocessing is to reduce words to their base or root form, known as the lemma. Unlike stemming, which simply chops off suffixes (sometimes resulting in non-dictionary words), lemmatization ensures that the derived form is a valid word in the language, making it more meaningful for analysis. Here‚Äôs how lemmatization serves various purposes in text preprocessing:\n",
        "\n",
        "1. Standardizing Word Forms:\n",
        "Lemmatization converts different forms of a word to a single, standardized lemma. For example, the words \"running,\" \"ran,\" and \"runs\" would all be reduced to the lemma \"run.\" This helps in treating various forms of a word as a single entity, making it easier to analyze word frequency, meaning, and relationships.\n",
        "2. Improving Accuracy of Text Analysis:\n",
        "By reducing words to their base form, lemmatization minimizes noise caused by morphological variations (like tense, number, or part of speech). This ensures that the analysis focuses on the core meaning of the word, leading to better performance in tasks like text classification, sentiment analysis, and topic modeling.\n",
        "3. Handling Synonyms:\n",
        "Lemmatization can help identify synonyms that have different inflected forms. For instance, \"better\" and \"good\" might be reduced to their lemma, helping to group words with similar meanings, thus enhancing the model's ability to recognize them as equivalent.\n",
        "4. Dealing with Inflection:\n",
        "Inflection refers to the changes in the form of a word based on grammatical features such as tense, number, or gender. Lemmatization handles these variations by reducing words to their dictionary form. For example:\n",
        "\"mice\" ‚Üí \"mouse\"\n",
        "\"children\" ‚Üí \"child\"\n",
        "\"was\" ‚Üí \"be\" This allows models to focus on the core meaning without being affected by different grammatical forms.\n",
        "5. Reducing Vocabulary Size:\n",
        "Lemmatization reduces the size of the vocabulary by consolidating multiple word forms into one lemma. This is useful in tasks like document classification or information retrieval, where reducing the dimensionality of the text can improve performance and efficiency.\n",
        "6. Improving Machine Learning Models:\n",
        "In machine learning, models often work better with a reduced and consistent feature set. Lemmatization ensures that related words (such as \"run,\" \"running,\" and \"runner\") are treated as a single feature, which helps reduce overfitting and enhances generalization, making models more effective in handling unseen data."
      ],
      "metadata": {
        "id": "gcYmTUHZS0RE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practical Questions"
      ],
      "metadata": {
        "id": "cOwXgVqJS0N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.1 How can you perform word tokenization using NLTK?\n",
        "\n",
        "# Ans :-\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')  # Download the pre-trained tokenizer models\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Hello, world! This is an example sentence.\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "['Hello', ',', 'world', '!', 'This', 'is', 'an', 'example', 'sentence', '.']\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "text = \"Hello, world! This is an example sentence.\"\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "['Hello', 'world', 'This', 'is', 'an', 'example', 'sentence']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is51ylzjSzBw",
        "outputId": "70fbb53b-2cde-47f3-e218-b793ecb28a6b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '!', 'This', 'is', 'an', 'example', 'sentence', '.']\n",
            "['Hello', 'world', 'This', 'is', 'an', 'example', 'sentence']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'world', 'This', 'is', 'an', 'example', 'sentence']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.2 How can you perform sentence tokenization using NLTK?\n",
        "# Ans :-\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')  # Download the pre-trained sentence tokenizer models\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"Hello world! This is a sentence tokenization example. Let's see how it works.\"\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(sentences)\n",
        "['Hello world!', 'This is a sentence tokenization example.', \"Let's see how it works.\"]\n",
        "text = \"Mr. Smith went to the market. Dr. Adams called him later. He said: 'I'll see you at 5 p.m.'\"\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "print(sentences)\n",
        "['Mr. Smith went to the market.', 'Dr. Adams called him later.', \"He said: 'I'll see you at 5 p.m.'\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXFO72LEcgdM",
        "outputId": "ecd0757d-352c-4802-a09c-ab50d46ecac6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "['Hello world!', 'This is a sentence tokenization example.', \"Let's see how it works.\"]\n",
            "['Mr. Smith went to the market.', 'Dr. Adams called him later.', \"He said: 'I'll see you at 5 p.m.'\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mr. Smith went to the market.',\n",
              " 'Dr. Adams called him later.',\n",
              " \"He said: 'I'll see you at 5 p.m.'\"]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.3 How can you remove stopwords from a sentence?\n",
        "# Ans :-\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')  # Download the stopwords corpus\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Example sentence\n",
        "text = \"This is an example sentence demonstrating how to remove stopwords.\"\n",
        "\n",
        "# Tokenize the sentence into words\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Get the list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Filter out stopwords from the tokenized words\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "print(filtered_words)\n",
        "\n",
        "['example', 'sentence', 'demonstrating', 'remove', 'stopwords', '.']\n",
        "\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words and word.isalpha()]\n",
        "\n",
        "print(filtered_words)\n",
        "\n",
        "['example', 'sentence', 'demonstrating', 'remove', 'stopwords']\n",
        "\n",
        "stop_words = set(stopwords.words('spanish'))  # For Spanish stopwords\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElxU6jJLdKSE",
        "outputId": "532c6193-8602-4a80-fdfa-f07c56da1e30"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "['example', 'sentence', 'demonstrating', 'remove', 'stopwords', '.']\n",
            "['example', 'sentence', 'demonstrating', 'remove', 'stopwords']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.4  How can you perform stemming on a word?\n",
        "# Ans :-\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')  # Download the punkt tokenizer models (if not already downloaded)\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the stemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Example word\n",
        "word = \"running\"\n",
        "\n",
        "# Perform stemming\n",
        "stemmed_word = ps.stem(word)\n",
        "\n",
        "print(stemmed_word)\n",
        "run\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "# Initialize the Lancaster stemmer\n",
        "ls = LancasterStemmer()\n",
        "\n",
        "# Perform stemming with LancasterStemmer\n",
        "stemmed_word = ls.stem(word)\n",
        "\n",
        "print(stemmed_word)\n",
        "\n",
        "words = [\"running\", \"jumps\", \"easily\", \"better\", \"happily\"]\n",
        "\n",
        "# Apply stemming to each word\n",
        "stemmed_words = [ps.stem(word) for word in words]\n",
        "\n",
        "print(stemmed_words)\n",
        "\n",
        "['run', 'jump', 'easili', 'better', 'happili']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "KY8mYlrJdidB",
        "outputId": "6dcc629c-8a2c-438e-aa08-117d76477c87"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "run\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'run' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-f60fc8522a4d>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLancasterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'run' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.5 How can you perform lemmatization on a word?\n",
        "#Ans :-\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('wordnet')  # Download WordNet for lemmatization\n",
        "nltk.download('punkt_tab')    # Download punkt for tokenization (if necessary)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example word\n",
        "word = \"running\"\n",
        "\n",
        "# Perform lemmatization (default POS is noun)\n",
        "lemma = lemmatizer.lemmatize(word)\n",
        "\n",
        "print(lemma)\n",
        "\n",
        "# Perform lemmatization with POS as verb\n",
        "lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "\n",
        "print(lemma)\n",
        "\n",
        "# Example adjective word\n",
        "adjective = \"better\"\n",
        "\n",
        "# Lemmatize with POS as adjective\n",
        "lemma_adj = lemmatizer.lemmatize(adjective, pos='a')\n",
        "\n",
        "print(lemma_adj)\n",
        "\n",
        "words = [\"running\", \"jumps\", \"better\", \"happily\"]\n",
        "\n",
        "# Apply lemmatization to each word (as verb for this example)\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "\n",
        "print(lemmatized_words)\n",
        "['run', 'jump', 'better', 'happily']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eeauRPFd4ti",
        "outputId": "ddbd68b7-43d5-4aec-b907-917a86e8aa24"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "running\n",
            "run\n",
            "good\n",
            "['run', 'jump', 'better', 'happily']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['run', 'jump', 'better', 'happily']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.6 How can you normalize a text by converting it to lowercase and removing punctuation?\n",
        "#Ans :-\n",
        "!pip install nltk\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt_tab')  # Download the punkt tokenizer (if needed)\n",
        "import string\n",
        "\n",
        "# Example text\n",
        "text = \"Hello, world! This is an example sentence.\"\n",
        "\n",
        "# Convert text to lowercase\n",
        "text_lower = text.lower()\n",
        "\n",
        "# Remove punctuation using str.translate\n",
        "text_normalized = text_lower.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "print(text_normalized)\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Example text\n",
        "text = \"Hello, world! This is an example sentence.\"\n",
        "\n",
        "# Convert text to lowercase and tokenize\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "# Remove punctuation\n",
        "tokens_without_punctuation = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "# Join the words back into a single string\n",
        "text_normalized = ' '.join(tokens_without_punctuation)\n",
        "\n",
        "print(text_normalized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gglsaUkuejR5",
        "outputId": "6c3b939b-8b7a-449a-a42c-1fdfa8e60df4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "hello world this is an example sentence\n",
            "hello world this is an example sentence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.7 How can you create a co-occurrence matrix for words in a corpus?\n",
        "#Ans :-\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "import string\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Example corpus (a small set of sentences)\n",
        "corpus = [\n",
        "    \"The quick brown fox jumped over the lazy dog.\",\n",
        "    \"The fox is quick and clever.\",\n",
        "    \"Dogs are great companions to foxes.\",\n",
        "]\n",
        "\n",
        "# Preprocess the corpus: Tokenize, remove stopwords and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = []\n",
        "\n",
        "# Tokenizing and normalizing (converting to lowercase and removing stopwords & punctuation)\n",
        "for sentence in corpus:\n",
        "    words = word_tokenize(sentence.lower())  # Convert to lowercase and tokenize\n",
        "    words = [word for word in words if word not in stop_words and word not in string.punctuation]\n",
        "    tokens.append(words)\n",
        "\n",
        "# Define a function to create the co-occurrence matrix\n",
        "def create_cooccurrence_matrix(tokens, window_size=2):\n",
        "    cooccurrence_matrix = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    # Iterate through each sentence's tokens\n",
        "    for sentence in tokens:\n",
        "        # Create co-occurrence pairs within a window size\n",
        "        for i, word in enumerate(sentence):\n",
        "            # Define the window of words around the current word\n",
        "            start = max(i - window_size, 0)\n",
        "            end = min(i + window_size + 1, len(sentence))\n",
        "\n",
        "            # Count the co-occurrence of each word with others within the window\n",
        "            for j in range(start, end):\n",
        "                if i != j:  # Avoid counting the word with itself\n",
        "                    cooccurrence_matrix[word][sentence[j]] += 1\n",
        "\n",
        "    return cooccurrence_matrix\n",
        "\n",
        "# Create the co-occurrence matrix\n",
        "cooccurrence_matrix = create_cooccurrence_matrix(tokens, window_size=2)\n",
        "\n",
        "# Print the co-occurrence matrix\n",
        "for word, neighbors in cooccurrence_matrix.items():\n",
        "    print(f\"Word: {word}\")\n",
        "    for neighbor, count in neighbors.items():\n",
        "        print(f\"  {neighbor}: {count}\")\n",
        "Word: quick\n",
        "brown: 1\n",
        "fox: 1\n",
        "jumped: 1\n",
        "the: 2\n",
        "Word: brown\n",
        "quick: 1\n",
        "fox: 1\n",
        "jumped: 1\n",
        "Word: fox\n",
        "quick: 1\n",
        "brown: 1\n",
        "jumped: 1\n",
        "the: 1\n",
        "is: 1\n",
        "clever: 1\n",
        "dogs: 1\n",
        "companions: 1\n",
        "Word: jumped\n",
        "quick: 1\n",
        "brown: 1\n",
        "fox: 1\n",
        "Word: the\n",
        "quick: 2\n",
        "fox: 2\n",
        "jumped: 1\n",
        "lazy: 1\n",
        "Word: lazy\n",
        "dog: 1\n",
        "Word: dog\n",
        "lazy: 1\n",
        "Word: is\n",
        "fox: 1\n",
        "quick: 1\n",
        "clever: 1\n",
        "Word: clever\n",
        "fox: 1\n",
        "is: 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "KJeiUiSOe2xZ",
        "outputId": "f7579e78-f268-44f3-d670-d2328718710b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-33-3449e1456acc>, line 76)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-3449e1456acc>\"\u001b[0;36m, line \u001b[0;32m76\u001b[0m\n\u001b[0;31m    is: 1\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.8 How can you apply a regular expression to extract all email addresses from a text\n",
        "# Ans :-\n",
        "import re\n",
        "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "import re\n",
        "\n",
        "# Example text containing email addresses\n",
        "text = \"\"\"\n",
        "Hello, you can contact us at support@example.com for more information.\n",
        "Alternatively, reach out to john.doe123@gmail.com or jane_doe@company.org.\n",
        "Thanks!\n",
        "\"\"\"\n",
        "\n",
        "# Regular expression pattern to match email addresses\n",
        "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "# Find all email addresses in the text\n",
        "email_addresses = re.findall(email_pattern, text)\n",
        "\n",
        "# Print the extracted email addresses\n",
        "print(email_addresses)\n",
        "['support@example.com', 'john.doe123@gmail.com', 'jane_doe@company.org']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8isTKaKmfVbp",
        "outputId": "e44f7307-e1bc-48ba-808f-789eae677f78"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['support@example.com', 'john.doe123@gmail.com', 'jane_doe@company.org']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['support@example.com', 'john.doe123@gmail.com', 'jane_doe@company.org']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.9 How can you perform word embedding using Word2Vec?\n",
        "# Ans :-\n",
        "!pip install gensim\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt_tab')\n",
        "# Example text corpus\n",
        "corpus = [\n",
        "    \"Word embeddings are a type of word representation that allows words to be represented in a continuous vector space.\",\n",
        "    \"The word2vec algorithm uses neural networks to map words to vectors.\",\n",
        "    \"Word2Vec creates word embeddings by learning from the context of words in a large corpus.\"\n",
        "]\n",
        "\n",
        "# Preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text and remove punctuation and stopwords\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "    return tokens\n",
        "\n",
        "tokenized_corpus = [preprocess_text(text) for text in corpus]\n",
        "# Train the Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, sg=1)\n",
        "# Access the vector for a word\n",
        "vector = model.wv['word2vec']  # The vector for the word 'word2vec'\n",
        "\n",
        "print(\"Vector for 'word2vec':\")\n",
        "print(vector)\n",
        "\n",
        "# Find words similar to 'word2vec'\n",
        "similar_words = model.wv.most_similar('word2vec', topn=5)\n",
        "print(\"\\nMost similar words to 'word2vec':\")\n",
        "print(similar_words)\n",
        "Vector for 'word2vec':\n",
        "[ 0.1234, -0.5432, 0.9876, ..., -0.4321]\n",
        "\n",
        "Most similar words to 'word2vec':\n",
        "[('embeddings', 0.8523), ('vector', 0.7432), ('neural', 0.6890), ...]\n",
        "# Save the model\n",
        "model.save(\"word2vec_model\")\n",
        "\n",
        "# Load the model\n",
        "loaded_model = Word2Vec.load(\"word2vec_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "6jJDSAT-fqpR",
        "outputId": "85f3dd33-6229-4a1d-f882-ea2694f75844"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-34-d60f7404d00b>, line 39)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-d60f7404d00b>\"\u001b[0;36m, line \u001b[0;32m39\u001b[0m\n\u001b[0;31m    Vector for 'word2vec':\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.10 How can you use Doc2Vec to embed documents?\n",
        "# Ans :-\n",
        "!pip install gensim\n",
        "import gensim\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt_tab')\n",
        "# Example documents\n",
        "corpus = [\n",
        "    \"Doc2Vec is a method for representing documents as vectors.\",\n",
        "    \"It extends Word2Vec to handle entire documents instead of just words.\",\n",
        "    \"Document embeddings can be used in various NLP tasks like document similarity and clustering.\"\n",
        "]\n",
        "\n",
        "# Preprocess the corpus (tokenization and lowercasing)\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]  # Remove punctuation\n",
        "    return tokens\n",
        "\n",
        "# Tokenize the documents\n",
        "tokenized_corpus = [preprocess_text(doc) for doc in corpus]\n",
        "# Convert the corpus into TaggedDocument format\n",
        "tagged_corpus = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(tokenized_corpus)]\n",
        "# Train the Doc2Vec model\n",
        "model = Doc2Vec(vector_size=50, window=2, min_count=1, workers=4, epochs=100)\n",
        "model.build_vocab(tagged_corpus)  # Build the vocabulary from the tagged corpus\n",
        "model.train(tagged_corpus, total_examples=model.corpus_count, epochs=model.epochs)  # Train the model\n",
        "# Get the vector representation for a specific document\n",
        "vector = model.infer_vector(tokenized_corpus[0])  # Embed the first document\n",
        "print(\"Vector for Document 0:\")\n",
        "print(vector)\n",
        "# Find the most similar documents to a given document\n",
        "similar_docs = model.dv.most_similar([model.infer_vector(tokenized_corpus[0])], topn=3)\n",
        "print(\"\\nMost similar documents to Document 0:\")\n",
        "print(similar_docs)\n",
        "Vector for Document 0:\n",
        " [ 0.037  -0.043  0.093  ... ]\n",
        " Most similar documents to Document 0:\n",
        "  [('0', 0.9999284744262695), ('2', 0.9843252892494202), ('1', 0.9787265062332153)]\n",
        "# Save the model\n",
        "model.save(\"doc2vec_model\")\n",
        "\n",
        "# Load the model\n",
        "loaded_model = Doc2Vec.load(\"doc2vec_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "gC1Gv3Txf9Sx",
        "outputId": "6e0aacc5-b4e1-4a05-bdbf-6ffcb0d83dd1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-36-fefcac5b0c2c>, line 41)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-fefcac5b0c2c>\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    Vector for Document 0:\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.11 : How can you perform part-of-speech tagging?\n",
        "# Ans :-\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "text = \"Natural language processing is a fascinating field of study.\"\n",
        "tokens = word_tokenize(text)\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "print(tagged_tokens)\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural language processing is a fascinating field of study.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Perform POS tagging\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "# Output the POS tags\n",
        "print(tagged_tokens)\n",
        "[('Natural', 'J'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('fascinating', 'JJ'), ('field', 'NN'), ('of', 'IN'), ('study', 'NN'), ('.', '.')]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "id": "4G02UgvCgYui",
        "outputId": "e0c91920-888c-44cc-c1ce-0b1d1f5ecdf3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-f6c86aeb50dc>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Natural language processing is a fascinating field of study.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load, lang)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tagdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mload_from_json\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# Automatically find path to the tagger if location is not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"taggers/averaged_perceptron_tagger_{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTAGGER_JSONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.12 How can you find the similarity between two sentences using cosine similarity?\n",
        "# Ans :- pip install scikit-learn\n",
        "!pip install scikit-learn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "sentence1 = \"I love programming in Python.\"\n",
        "sentence2 = \"Python programming is fun.\"\n",
        "# Create a TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the sentences into vectors\n",
        "tfidf_matrix = vectorizer.fit_transform([sentence1, sentence2])\n",
        "# Compute cosine similarity between the two sentences\n",
        "similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "\n",
        "print(\"Cosine Similarity:\", similarity[0][0])\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sentences to compare\n",
        "sentence1 = \"I love programming in Python.\"\n",
        "sentence2 = \"Python programming is fun.\"\n",
        "\n",
        "# Create TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Convert sentences into TF-IDF vectors\n",
        "tfidf_matrix = vectorizer.fit_transform([sentence1, sentence2])\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "\n",
        "# Output the similarity score\n",
        "print(\"Cosine Similarity:\", similarity[0][0])\n",
        "Cosine Similarity: 0.7362076011879446\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "45gnNZQSgsRx",
        "outputId": "d64d8a2c-c3ba-45f6-f83e-8ca40ceda41f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-38-768e771adef2>, line 35)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-38-768e771adef2>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    Cosine Similarity: 0.7362076011879446\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.13 How can you extract named entities from a sentence?\n",
        "#Ans :-\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"Barack Obama was born in Honolulu, Hawaii on August 4, 1961.\"\n",
        "\n",
        "# Step 1: Tokenize the sentence\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# Step 2: Perform POS tagging\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "# Step 3: Perform Named Entity Recognition (NER)\n",
        "named_entities = ne_chunk(tagged_tokens)\n",
        "\n",
        "# Output the named entities\n",
        "print(named_entities)\n",
        "(S(GPE Barack/NNP Obama/NNP)\n",
        "  was/VBD\n",
        "  born/VBN\n",
        "  in/IN\n",
        "  (GPE Honolulu/NNP)\n",
        "  ,/,\n",
        "  (GPE Hawaii/NNP)\n",
        "  on/IN\n",
        "  (GPE August/NNP 4/CD ,/, 1961/CD ./.))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "p-6ZaxlKhHGU",
        "outputId": "90b267ed-df05-4338-8507-013c8d89371b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (<ipython-input-40-74747d6a9657>, line 31)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-74747d6a9657>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    (S(GPE Barack/NNP Obama/NNP)\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.14 : How can you split a large document into smaller chunks of text?\n",
        "# Ans :-\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Sample large document\n",
        "document = \"\"\"\n",
        "Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through language. It involves several tasks such as text analysis, speech recognition, and language generation.\n",
        "Machine learning techniques have become a central part of NLP, enabling computers to learn from large amounts of data. Deep learning, a subfield of machine learning, has shown remarkable progress in tasks like machine translation, text summarization, and sentiment analysis.\n",
        "\"\"\"\n",
        "\n",
        "# Split the document into sentences\n",
        "sentences = sent_tokenize(document)\n",
        "\n",
        "# Output the sentences\n",
        "for idx, sentence in enumerate(sentences):\n",
        "    print(f\"Sentence {idx+1}: {sentence}\")\n",
        "Sentence 1: Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through language.\n",
        "Sentence 2: It involves several tasks such as text analysis, speech recognition, and language generation.\n",
        "Sentence 3: Machine learning techniques have become a central part of NLP, enabling computers to learn from large amounts of data.\n",
        "Sentence 4: Deep learning, a subfield of machine learning, has shown remarkable progress in tasks like machine translation, text summarization, and sentiment analysis.\n",
        "def split_document_by_length(text, chunk_size=100):\n",
        "    words = text.split()\n",
        "    chunks = [words[i:i + chunk_size] for i in range(0, len(words), chunk_size)]\n",
        "    return [' '.join(chunk) for chunk in chunks]\n",
        "\n",
        "# Sample large document\n",
        "document = \"\"\"\n",
        "Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through language. It involves several tasks such as text analysis, speech recognition, and language generation. Machine learning techniques have become a central part of NLP, enabling computers to learn from large amounts of data. Deep learning, a subfield of machine learning, has shown remarkable progress in tasks like machine translation, text summarization, and sentiment analysis.\n",
        "\"\"\"\n",
        "\n",
        "# Split the document into chunks of 50 words\n",
        "chunks = split_document_by_length(document, chunk_size=50)\n",
        "\n",
        "# Output the chunks\n",
        "for idx, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {idx+1}: {chunk}\")\n",
        "Chunk 1: Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through language. It involves several tasks such as text analysis, speech recognition, and language generation.\n",
        "Chunk 2: Machine learning techniques have become a central part of NLP, enabling computers to learn from large amounts of data. Deep learning, a subfield of machine learning, has shown remarkable progress in tasks like machine translation, text summarization, and sentiment analysis.\n",
        "# Sample large document with paragraphs\n",
        "document = \"\"\"\n",
        "Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through language. It involves several tasks such as text analysis, speech recognition, and language generation.\n",
        "\n",
        "Machine learning techniques have become a central part of NLP, enabling computers to learn from large amounts of data. Deep learning, a subfield of machine learning, has shown remarkable progress in tasks like machine translation, text summarization, and sentiment analysis.\n",
        "\"\"\"\n",
        "\n",
        "# Split the document by paragraphs (assuming paragraphs are separated by newlines)\n",
        "paragraphs = document.split(\"\\n\\n\")\n",
        "\n",
        "# Output the paragraphs\n",
        "for idx, paragraph in enumerate(paragraphs):\n",
        "    print(f\"Paragraph {idx+1}: {paragraph}\")\n",
        "Paragraph 1: Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through language. It involves several tasks such as text analysis, speech recognition, and language generation.\n",
        "Paragraph 2: Machine learning techniques have become a central part of NLP, enabling computers to learn from large amounts of data. Deep learning, a subfield of machine learning, has shown remarkable progress in tasks like machine translation, text summarization, and sentiment analysis.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "1l4mD4QphnfK",
        "outputId": "b3294589-b01b-4c61-c498-de844ba50555"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-41-86ab2f38c0b7>, line 20)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-41-86ab2f38c0b7>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    Sentence 1: Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through language.\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.15 How can you calculate the TF-IDF (Term Frequency - Inverse Document Frequency) for a set of documents?\n",
        "# Ans :-\n",
        "!pip install scikit-learn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The dog sat on the log.\",\n",
        "    \"The cat chased the dog.\"\n",
        "]\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the documents into TF-IDF matrix\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert the TF-IDF matrix to a dense format and display it\n",
        "dense_tfidf = tfidf_matrix.todense()\n",
        "\n",
        "# Convert to a DataFrame for better readability\n",
        "import pandas as pd\n",
        "df_tfidf = pd.DataFrame(dense_tfidf, columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Output the TF-IDF values\n",
        "print(df_tfidf)\n",
        "cat           chased       dog       log       mat       on       sat       the\n",
        "0  0.577350  0.000000  0.000000  0.000000  0.577350  0.577350  0.577350  0.577350\n",
        "1  0.000000  0.000000  0.577350  0.577350  0.000000  0.577350  0.577350  0.577350\n",
        "2  0.577350  0.577350  0.577350  0.000000  0.000000  0.000000  0.577350  0.577350\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "nHFbb5-4h9OZ",
        "outputId": "fc405caf-39a2-4ce5-b97c-4abb37d137bc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-44-5b8c8eafff23>, line 28)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-44-5b8c8eafff23>\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    cat           chased       dog       log       mat       on       sat       the\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.16 How can you apply tokenization, stopword removal, and stemming in one go?\n",
        "# Ans :-\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to apply tokenization, stopword removal, and stemming\n",
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens_no_stopwords = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens_no_stopwords]\n",
        "\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Example text\n",
        "text = \"The cats are running fast on the field.\"\n",
        "\n",
        "# Apply preprocessing\n",
        "processed_text = preprocess_text(text)\n",
        "\n",
        "# Output the result\n",
        "print(\"Processed Text:\", processed_text)\n",
        "Processed Text: ['cat', 'run', 'fast', 'field']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "Bg-vBFH1ieJ5",
        "outputId": "e53fa37e-abe2-48f0-b000-e81ca6592b12"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-45-f4e73a2bc689>, line 35)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-45-f4e73a2bc689>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    Processed Text: ['cat', 'run', 'fast', 'field']\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.17 How can you visualize the frequency distribution of words in a sentence?\n",
        "# Ans :-\n",
        "import nltk\n",
        "from nltk.probability import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample sentence\n",
        "sentence = \"The cat sat on the mat and the cat looked at the mat.\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "# Create frequency distribution\n",
        "fdist = FreqDist(tokens)\n",
        "\n",
        "# Plot the frequency distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "fdist.plot(30, cumulative=False)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "KUjYUwIhipz5",
        "outputId": "987ac8ba-2868-4065-ca70-7aa5112fdb15"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "arc1DHQxiznJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}