{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUAqXfoYWzjnGG7pMhBl9x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nk28-byte/MY-ASSIGNMENT/blob/main/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.1 What does GAN stand for ,and what is it main purpose?\n",
        "\n",
        "Ans :- GAN stands for Generative Adversarial Network.\n",
        "\n",
        "Main Purpose:\n",
        "\n",
        "GANs are a type of machine learning framework designed to generate new data instances that resemble the training data. They achieve this through a competitive process between two neural networks:\n",
        "\n",
        "Generator: This network generates new data instances, such as images, text, or audio.\n",
        "Discriminator: This network evaluates the generated data and tries to distinguish between real and fake data.\n",
        "The generator and discriminator are trained together in an adversarial process. The generator aims to produce data that can fool the discriminator, while the discriminator strives to accurately classify real and fake data. This adversarial training leads to the generator becoming increasingly skilled at producing realistic data.\n",
        "\n",
        "Key Applications of GANs:\n",
        "\n",
        "Image Generation: Creating realistic images of faces, objects, or scenes.\n",
        "Data Augmentation: Generating synthetic data to enhance training datasets.\n",
        "Style Transfer: Transferring the style of one image to another.\n",
        "Super-resolution: Enhancing the resolution of low-resolution images.\n",
        "Text-to-Image Generation: Generating images based on text descriptions.\n",
        "\n",
        "\n",
        "\n",
        " Q.2 Explain the concept of 'dicriminator' in GANs.\n",
        "\n",
        " Ans :- The Discriminator in GANs\n",
        "\n",
        "In a Generative Adversarial Network (GAN), the discriminator is a neural network that acts as a critic, evaluating the authenticity of data samples. Its primary role is to distinguish between real data samples from the training dataset and fake data generated by the generator network.\n",
        "\n",
        "How it Works:\n",
        "\n",
        "Input: The discriminator receives both real data samples and fake data samples generated by the generator.\n",
        "Classification: It processes these inputs and outputs a probability score indicating the likelihood that the input is real.\n",
        "Feedback: The discriminator's output is used to train both the discriminator and the generator.\n",
        "If the discriminator correctly identifies a real sample as real and a fake sample as fake, it receives a reward.\n",
        "\n",
        "Q.3 How does GAN work?\n",
        "\n",
        "Ans ;- How GANs Work: A Step-by-Step Breakdown\n",
        "\n",
        "Generative Adversarial Networks (GANs) are a powerful class of machine learning models that pit two neural networks against each other in a zero-sum game. This adversarial process drives both networks to improve, ultimately leading to the generation of highly realistic data.\n",
        "\n",
        "The Two Key Players:\n",
        "\n",
        "Generator:\n",
        "\n",
        "Takes random noise as input.\n",
        "Generates new data instances, such as images, text, or audio.\n",
        "Aims to fool the discriminator by producing data that is indistinguishable from real data.\n",
        "Discriminator:\n",
        "\n",
        "Takes both real and fake data as input.\n",
        "Determines whether the input data is real or generated.\n",
        "Aims to accurately classify real and fake data.\n",
        "The Adversarial Process:\n",
        "\n",
        "Random Noise Input: The generator takes random noise as input and generates a new data sample.\n",
        "Discriminator Evaluation: The discriminator receives both real data samples from the training dataset and the fake data generated by the generator.\n",
        "\n",
        " It then classifies each sample as real or fake.\n",
        "\n",
        " Q.4 What is the generator role in GAN?\n",
        "\n",
        " Ans :- The Generator's Role in GANs\n",
        "\n",
        "In a Generative Adversarial Network (GAN), the generator is one of the two primary components. Its primary role is to generate new, synthetic data that is indistinguishable from real data.\n",
        "\n",
        "Key Function:\n",
        "\n",
        "Noise to Data: The generator takes random noise as input and transforms it into realistic data, such as images, text, or audio.\n",
        "Adversarial Learning: It is constantly competing with the discriminator. The generator's goal is to produce data that can fool the discriminator into believing it is real.\n",
        "How it Works:\n",
        "\n",
        "Random Noise Input: The generator receives a random noise vector as input.\n",
        "Data Generation: The generator processes this noise through a series of layers, progressively transforming it into a more complex and realistic output.\n",
        "Discriminator Evaluation: The generated data is fed to the discriminator, which classifies it as real or fake\n",
        "\n",
        "Q.5 What is the loss function and used in the training of GANs?\n",
        "\n",
        "Ans :- Loss Functions in GANs\n",
        "\n",
        "GANs typically employ a min-max loss function to train the generator and discriminator networks. This loss function is designed to create a competitive environment where both networks strive to improve.\n",
        "\n",
        "Discriminator Loss:\n",
        "\n",
        "The discriminator's loss function aims to maximize the probability of correctly classifying real data as real and fake data as fake. This can be expressed as:\n",
        "\n",
        "Loss_D = -E[log(D(x))] - E[log(1 - D(G(z)))]\n",
        "E[log(D(x))]: Encourages the discriminator to correctly classify real data.\n",
        "E[log(1 - D(G(z)))]: Encourages the discriminator to correctly classify fake data.\n",
        "Generator Loss:\n",
        "\n",
        "The generator's loss function aims to minimize the discriminator's ability to distinguish between real and fake data. This can be expressed as:\n",
        "\n",
        "Loss_G = -E[log(D(G(z)))]\n",
        "The generator tries to maximize the probability of the discriminator classifying its generated data as real.\n",
        "\n",
        "Training Process:\n",
        "\n",
        "Training the Discriminator:\n",
        "\n",
        "The discriminator is trained to maximize its loss function.\n",
        "It is fed both real data and fake data generated by the generator.\n",
        "The discriminator's weights are updated to improve its classification accuracy.\n",
        "Training the Generator:\n",
        "\n",
        "The generator is trained to minimize the discriminator's loss function.\n",
        "It generates fake data and feeds it to the discriminator.\n",
        "\n",
        "Q.6 What is the difference between a WGAN and a traditional GAN?\n",
        "\n",
        "Ans :- WGAN vs. Traditional GAN: A Comparative Analysis\n",
        "Traditional GANs\n",
        "\n",
        "Loss Function: Uses binary cross-entropy loss to train the discriminator to classify real and fake data.\n",
        "Discriminator's Role: The discriminator directly classifies data as real or fake.\n",
        "Training Stability: Can be challenging to train due to mode collapse and vanishing gradients.\n",
        "Wasserstein GAN (WGAN)\n",
        "\n",
        "Loss Function: Uses the Wasserstein distance, a metric that measures the distance between two probability distributions, as the loss function.\n",
        "Critic's Role: The discriminator, often referred to as a \"critic,\" estimates the Wasserstein distance between the real and generated data distributions.\n",
        "Training Stability: WGANs are more stable to train, reducing the risk of mode collapse.\n",
        "Improved Gradient Flow: The Wasserstein distance provides smoother gradients, allowing for more effective training.\n",
        "Weight Clipping: WGANs often use weight clipping to stabilize training, but this can limit their performance.\n",
        "\n",
        "\n",
        "\n",
        "Q.7 How does the training of the generator differ from that of the discriminator?\n",
        "\n",
        "Ans :- Generator vs. Discriminator Training in GANs\n",
        "In a Generative Adversarial Network (GAN), the generator and discriminator are trained in an adversarial process, each aiming to outsmart the other.\n",
        "\n",
        "Generator's Training:\n",
        "\n",
        "Goal: To produce data that is indistinguishable from real data.\n",
        "Loss Function: The generator's loss function is designed to maximize the probability that the discriminator classifies its generated data as real. This is typically achieved by minimizing the negative log-likelihood of the discriminator's output for generated data.\n",
        "Training Process:\n",
        "Random Noise Input: The generator takes random noise as input.\n",
        "Data Generation: It processes the noise through its neural network to generate a fake data sample.\n",
        "Discriminator Evaluation: The discriminator classifies the fake data as real or fake.\n",
        "Gradient Update: Based on the discriminator's feedback, the generator's parameters are updated using backpropagation to produce more realistic data in the future.\n",
        "Discriminator's Training:\n",
        "\n",
        "Goal: To accurately distinguish between real and fake data.\n",
        "Loss Function: The discriminator's loss function aims to minimize the error in classifying real and fake data. This is typically achieved by maximizing the log-likelihood of correctly classifying real data and minimizing the log-likelihood of correctly classifying fake data.\n",
        "Training Process:\n",
        "Data Input: The discriminator receives both real data samples from the training set and fake data generated by the generator.\n",
        "Classification: It classifies each sample as real or fake.\n",
        "Gradient Update: Based on the correct classifications and the generator's ability to fool it, the discriminator's parameters are updated to improve its classification accuracy.\n",
        "\n",
        "\n",
        " Q.8 What is DCGAN,and how is it different from a traditional GAN?\n",
        "\n",
        " Ans :- DCGAN: A Deep Convolutional GAN\n",
        "\n",
        "A Deep Convolutional Generative Adversarial Network (DCGAN) is a specific architecture of a GAN that utilizes convolutional neural networks (CNNs) for both the generator and the discriminator. This architectural choice is particularly effective for image generation tasks.\n",
        "\n",
        "Key Differences between DCGAN and Traditional GANs:\n",
        "\n",
        "Architecture:\n",
        "\n",
        "Traditional GANs: Often use fully connected layers in both the generator and discriminator.\n",
        "DCGANs: Employ convolutional and transposed convolutional layers in the generator and discriminator, respectively. This allows for better capture of spatial information and hierarchical feature learning.\n",
        "Stability:\n",
        "\n",
        "Traditional GANs: Can be notoriously difficult to train, often suffering from mode collapse and vanishing gradients.\n",
        "DCGANs: Incorporate techniques like batch normalization and specific activation functions (e.g., LeakyReLU) to stabilize the training process.\n",
        "Image Quality:\n",
        "\n",
        "DCGANs: Generally produce higher-quality, more realistic images due to their ability to learn complex image patterns and textures.\n",
        "Benefits of DCGANs:\n",
        "\n",
        "Improved Image Quality: DCGANs excel at generating high-resolution, photo-realistic images.\n",
        "Stable Training: The use of convolutional layers and specific architectural choices contributes to more stable training.\n",
        "Versatility: DCGANs can be applied to various image generation tasks, such as image-to-image translation, style transfer, and super-resolution.\n",
        "\n",
        "Q.9 Explain the concept of controllable generation in the context of GANs.\n",
        "\n",
        "Ans :- Controllable Generation in GANs\n",
        "\n",
        "Controllable generation in GANs refers to the ability to manipulate the output of a GAN by providing specific inputs or constraints. This allows for more precise control over the generated data, enabling the creation of specific types of images or other data.\n",
        "\n",
        "Key Techniques for Controllable Generation:\n",
        "\n",
        "Conditional GANs (cGANs):\n",
        "\n",
        "Introduce a class label or other conditioning information as additional input to both the generator and discriminator.\n",
        "This allows the generator to produce images that correspond to specific classes or conditions.\n",
        "Latent Space Manipulation:\n",
        "\n",
        "The latent space of a GAN is a high-dimensional space where each point represents a potential image.\n",
        "By manipulating specific dimensions of the latent space, we can control certain attributes of the generated image, such as pose, expression, or style.\n",
        "Gradient-Based Methods:\n",
        "\n",
        "Gradient-based methods involve calculating the gradient of a specific feature with respect to the latent code.\n",
        "By manipulating the latent code in the direction of the gradient, we can control the desired feature in the generated image.\n",
        "Style Transfer:\n",
        "\n",
        "Style transfer techniques can be combined with GANs to control the style of the generated images.\n",
        "By providing a style reference image, the generator can produce images with the same style but different content.\n",
        "\n",
        "\n",
        "Q.10 What is the primary goal of training a GAN?\n",
        "\n",
        "Ans :- The primary goal of training a GAN is to train a generator network that can produce realistic, synthetic data that is indistinguishable from real data.\n",
        "\n",
        "To achieve this, the generator and discriminator networks engage in a competitive process:\n",
        "\n",
        "Generator:\n",
        "\n",
        "Takes random noise as input.\n",
        "Generates new data instances (e.g., images, text, audio).\n",
        "Aims to fool the discriminator by producing data that is as realistic as possible.\n",
        "Discriminator:\n",
        "\n",
        "Takes both real and fake data as input.\n",
        "Distinguishes between real and fake data.\n",
        "Aims to correctly classify real data as real and fake data as fake.\n",
        "Through this adversarial process, the generator learns to produce increasingly realistic data, while the discriminator becomes better at distinguishing between real and fake data.\n",
        "\n",
        "\n",
        "Q.11 What are the limitations of GANs?\n",
        "\n",
        "Ans :- Limitations of GANs\n",
        "\n",
        "Despite their impressive capabilities, GANs have several limitations:\n",
        "\n",
        "Training Instability:\n",
        "\n",
        "GANs are notoriously difficult to train, often leading to mode collapse or unstable behavior.\n",
        "The adversarial nature of the training process can make it challenging to balance the training of the generator and discriminator.\n",
        "Mode Collapse:\n",
        "\n",
        "This occurs when the generator produces a limited variety of samples, failing to capture the full diversity of the training data.\n",
        "This can lead to a lack of diversity in the generated samples.\n",
        "Evaluation Difficulty:\n",
        "\n",
        "Evaluating the quality of generated samples can be subjective and challenging.\n",
        "Traditional metrics like accuracy and loss may not be sufficient to assess the quality of generated data.\n",
        "\n",
        "\n",
        "Q.12 What are styleGANs , and what makes them unique?\n",
        "\n",
        "Ans :- StyleGAN is a type of Generative Adversarial Network (GAN) that has significantly advanced the state-of-the-art in image synthesis. It introduces a novel generator architecture that allows for fine-grained control over the generated images.\n",
        "\n",
        "Key Features of StyleGAN:\n",
        "\n",
        "Style-Based Generator Architecture:\n",
        "The generator uses a style-based architecture, where a latent code is mapped to a style code at each layer of the generator.\n",
        "This allows for independent control over different image attributes, such as pose, expression, and lighting.\n",
        "Progressive Growing:\n",
        "The generator starts with low-resolution images and gradually increases the resolution as training progresses.\n",
        "This allows for more efficient training and higher-quality images.\n",
        "Disentangled Latent Space:\n",
        "The latent space of StyleGAN is disentangled, meaning that different dimensions of the latent code control different aspects of the generated image.\n",
        "This enables fine-grained control over the generation process.\n",
        "What Makes StyleGAN Unique:\n",
        "\n",
        "High-Quality Image Generation: StyleGANs can generate incredibly realistic and diverse images, often indistinguishable from real photographs.\n",
        "Controllable Image Synthesis: By manipulating the latent codes, users can control various aspects of the generated images, such as pose, expression, and style.\n",
        "Efficient Training: The progressive growing technique allows for efficient training of large-scale models.\n",
        "\n",
        "\n",
        " Q.13 What is the role of noise in GAN?\n",
        "\n",
        " Noise in GANs: The Seed of Creativity\n",
        "\n",
        "In a Generative Adversarial Network (GAN), noise serves as the initial input to the generator network. It's a random vector that the generator processes to create realistic data, such as images, text, or audio.\n",
        "\n",
        "Key Roles of Noise:\n",
        "\n",
        "Diversity and Creativity:\n",
        "\n",
        "Noise introduces randomness and variability into the generation process, leading to a diverse range of outputs.\n",
        "This diversity helps the generator explore different creative possibilities and avoid generating repetitive or mundane samples.\n",
        "Breaking Symmetry:\n",
        "\n",
        "Noise can help break symmetries in the latent space, encouraging the generator to explore a wider range of possible outputs.\n",
        "This can lead to more diverse and interesting generated samples.\n",
        "Stabilizing Training:\n",
        "\n",
        "By adding noise to the input, the generator can avoid getting stuck in local minima during the training process.\n",
        "This can help to stabilize the training and improve the quality of the generated samples.\n",
        "\n",
        "\n",
        " Q.14 How does the loss function in a WGAN improve training stability?\n",
        "\n",
        " Ans :- Wasserstein GAN (WGAN) Loss Function and Improved Stability\n",
        "\n",
        "Traditional GANs use a binary cross-entropy loss function, which can lead to training instability, mode collapse, and vanishing gradients. WGANs address these issues by employing the Wasserstein distance as a loss function.\n",
        "\n",
        "Key Improvements:\n",
        "\n",
        "Smoother Gradients:\n",
        "\n",
        "The Wasserstein distance provides a smoother gradient, making the training process more stable.\n",
        "This helps to avoid the vanishing gradient problem, where gradients become too small to effectively update the model's parameters.\n",
        "Reduced Mode Collapse:\n",
        "\n",
        "The Wasserstein distance encourages the generator to explore a wider range of modes in the data distribution, reducing the likelihood of mode collapse.\n",
        "This leads to more diverse and realistic generated samples.\n",
        "Better Convergence:\n",
        "\n",
        "The WGAN loss function allows for more efficient training, leading to faster convergence and improved performance.\n",
        "\n",
        "\n",
        " Q.15 Describe the archeitectur of a typical GAN.\n",
        "\n",
        " Ans :- A typical GAN architecture consists of two main components: a generator and a discriminator.\n",
        "\n",
        "Generator:\n",
        "\n",
        "The generator's role is to create realistic data samples. It takes random noise as input and processes it through a series of layers to produce an output that resembles real data. The architecture of the generator often involves:\n",
        "\n",
        "Dense Layers: These layers transform the random noise into a higher-dimensional representation.\n",
        "Convolutional Layers: These layers are used to gradually increase the spatial dimensions of the generated data, often starting from a low-resolution image and progressively upsampling it.\n",
        "Activation Functions: Non-linear activation functions like ReLU or LeakyReLU are used to introduce non-linearity into the network.\n",
        "Discriminator:\n",
        "\n",
        "The discriminator's role is to distinguish between real and fake data. It takes both real data samples and generated samples as input and outputs a probability indicating whether the input is real or fake. The architecture of the discriminator often involves:\n",
        "\n",
        "Convolutional Layers: These layers extract features from the input image, both real and fake.\n",
        "Pooling Layers: These layers reduce the spatial dimensions of the feature maps.\n",
        "Fully Connected Layers: These layers classify the input as real or fake.\n",
        "Training Process:\n",
        "\n",
        "The generator and discriminator are trained in an adversarial manner. The generator tries to produce data that can fool the discriminator, while the discriminator tries to accurately distinguish between real and fake data. This adversarial process drives both networks to improve, leading to the generation of increasingly realistic data.\n",
        "\n",
        "\n",
        "Q.16 What challenges do GANs face during training ,and how can they be addressed?\n",
        "\n",
        "Ans :- Challenges in Training GANs and Their Solutions\n",
        "GANs, while powerful, present several challenges during training:\n",
        "\n",
        "1. Mode Collapse:\n",
        "\n",
        "Problem: The generator may converge to a limited set of modes, producing repetitive samples.\n",
        "Solutions:\n",
        "Feature Matching Loss: Encourages the generator to match the feature statistics of real and fake data.\n",
        "Spectral Normalization: Stabilizes training by controlling the spectral norm of the discriminator's weights.\n",
        "Progressive Growing of GANs (PGGAN): Starts with low-resolution images and gradually increases resolution, making training more stable.\n",
        "2. Vanishing Gradients:\n",
        "\n",
        "Problem: Gradients can become very small, hindering the learning process.\n",
        "Solutions:\n",
        "Proper Initialization: Careful initialization of weights can help alleviate vanishing gradients.\n",
        "Batch Normalization: Normalizes the input to each layer, improving training stability.\n",
        "Gradient Clipping: Limits the magnitude of gradients to prevent them from becoming too large or too small.\n",
        "3. Oscillating Training Dynamics:\n",
        "\n",
        "Problem: The generator and discriminator may enter an unstable cycle, where they overpower each other.\n",
        "Solutions:\n",
        "Adaptive Learning Rates: Adjusting learning rates during training can help stabilize the training process.\n",
        "Careful Hyperparameter Tuning: Experimenting with different hyperparameters, such as learning rate, batch size, and optimizer, can improve stability.\n",
        "Early Stopping: Monitoring training metrics and stopping the training process when performance plateaus can prevent overfitting and instability.\n",
        "\n",
        "\n",
        "Q.17 How Does DCGAN help improve image generation in GANs?\n",
        "\n",
        "Ans :- DCGANs significantly improve image generation in GANs by leveraging the power of convolutional neural networks. Here's how:\n",
        "\n",
        "1. Stable Training:\n",
        "\n",
        "Convolutional Layers: DCGANs use convolutional and transposed convolutional layers in the generator and discriminator, respectively. This architecture helps stabilize the training process, reducing the risk of mode collapse and vanishing gradients.\n",
        "Batch Normalization: By incorporating batch normalization in both the generator and discriminator, DCGANs further stabilize training and accelerate convergence.\n",
        "2. Improved Image Quality:\n",
        "\n",
        "Hierarchical Feature Learning: The convolutional layers in DCGANs allow for hierarchical feature learning, capturing low-level features (e.g., edges, textures) and gradually building up to high-level features (e.g., objects, scenes). This leads to more detailed and realistic image generation.\n",
        "Spatial Information Preservation: Convolutional layers are well-suited for processing spatial information, enabling DCGANs to generate images with coherent spatial structure and realistic details.\n",
        "3. Diverse Image Generation:\n",
        "\n",
        "Latent Space Manipulation: By manipulating the latent space input to the generator, DCGANs can generate a wide range of diverse images, including different styles, poses, and expressions.\n",
        "4. Scalability:\n",
        "\n",
        "DCGANs can be scaled to generate high-resolution images by increasing the depth and complexity of the network architecture. This allows for the creation of highly detailed and realistic images.\n",
        "\n",
        "\n",
        "Q.18 What are the key difference between a traditional GAN and a styleGAN?\n",
        "\n",
        "Ans :- Key Differences Between Traditional GANs and StyleGANs\n",
        "While both traditional GANs and StyleGANs are generative models capable of producing realistic images, StyleGANs offer significant improvements in terms of image quality, controllability, and training stability.\n",
        "\n",
        "Key Differences:\n",
        "\n",
        "Generator Architecture:\n",
        "\n",
        "Traditional GANs: Typically use a simple feedforward neural network architecture to generate images directly from random noise.\n",
        "StyleGANs: Employ a more sophisticated architecture that separates style and content information. This allows for more granular control over the generated images.\n",
        "Latent Space:\n",
        "\n",
        "Traditional GANs: The latent space is often less structured and can be difficult to interpret.\n",
        "StyleGANs: The latent space is more disentangled, allowing for independent control of various image attributes like pose, expression, and lighting.\n",
        "Image Quality:\n",
        "\n",
        "Traditional GANs: Can produce high-quality images, but often suffer from artifacts and lack of diversity.\n",
        "StyleGANs: Generate significantly higher-quality images with finer details and more realistic textures.\n",
        "Controllability:\n",
        "\n",
        "Traditional GANs: Offer limited control over the generated images. Manipulating the latent space often leads to unpredictable changes.\n",
        "StyleGANs: Allow for precise control over specific image attributes, such as facial features, hair style, and clothing.\n",
        "Training Stability:\n",
        "\n",
        "Traditional GANs: Can be challenging to train, often leading to mode collapse and unstable behavior.\n",
        "StyleGANs: Employ techniques like progressive growing and adaptive instance normalization to improve training stability and convergence.\n",
        "\n",
        "\n",
        " Q.19 How does the discriminator decide weathear an image is real or fake in a GAN?\n",
        "\n",
        " Ans :-  The discriminator in a GAN acts as a classifier, distinguishing between real and fake data. It learns to identify patterns and features that differentiate real data from generated data.\n",
        "\n",
        "Here's a breakdown of how the discriminator makes its decision:\n",
        "\n",
        "Feature Extraction: The discriminator extracts features from the input image (real or fake) using convolutional layers. These features capture the underlying patterns and structures in the image.\n",
        "Classification: The extracted features are fed through fully connected layers to classify the input as either real or fake. The final layer typically outputs a probability score between 0 and 1, where 1 indicates high confidence in the image being real.\n",
        "Loss Function: The discriminator is trained to minimize a loss function that penalizes incorrect classifications. This loss function encourages the discriminator to accurately distinguish between real and fake images.\n",
        "As the generator improves its ability to produce realistic images, the discriminator must also adapt to stay ahead. This adversarial process drives both networks to improve, leading to the generation of increasingly realistic data.\n",
        "\n",
        "It's important to note that the discriminator doesn't have access to the generator's internal workings.\n",
        "\n",
        "It simply learns to identify patterns in the data that distinguish real from fake. As the generator becomes more sophisticated, the discriminator must also learn to adapt and refine its classification criteria.\n",
        "\n",
        "\n",
        "Q.20 What is the main advantage of using GANs in image generation?\n",
        "\n",
        "Ans :- The primary advantage of GANs in image generation is their ability to produce highly realistic and diverse images.\n",
        "\n",
        "By pitting two neural networks against each other in an adversarial process, GANs can generate images that are often indistinguishable from real photographs.\n",
        "\n",
        "Here are some specific advantages of GANs in image generation:\n",
        "\n",
        "High-quality image generation: GANs can produce images with fine-grained details and realistic textures.\n",
        "Diverse image generation: GANs can generate a wide range of images, even for classes with limited training data.\n",
        "Controllable image generation: Techniques like StyleGAN allow for precise control over the generated images, such as manipulating specific attributes like pose, expression, or style.\n",
        "Data augmentation: GANs can be used to generate synthetic data to augment training datasets, improving the performance of other machine learning models.\n",
        "Image-to-image translation: GANs can be used to translate images from one domain to another, such as converting day images to night images or transforming photos into paintings.\n",
        "\n",
        "Q.21 How can GANs be used in real-world applications?\n",
        "\n",
        "Ans :- GANs have a wide range of real-world applications, here are some of the most prominent ones:\n",
        "\n",
        "Image Generation and Manipulation:\n",
        "\n",
        "Realistic Image Generation: Creating highly realistic images of people, objects, or scenes that don't exist in the real world.\n",
        "Image-to-Image Translation: Converting images from one domain to another, such as turning day images into night images or transforming photos into paintings.\n",
        "Image Super-resolution: Enhancing the resolution of low-resolution images.\n",
        "Image Inpainting: Filling in missing parts of an image.\n",
        "Data Augmentation:\n",
        "\n",
        "Generating Synthetic Data: Creating additional training data to improve the performance of machine learning models, especially for tasks with limited data.\n",
        "Medical Imaging:\n",
        "\n",
        "Medical Image Generation: Creating synthetic medical images for training and testing medical image analysis models.\n",
        "Medical Image Enhancement: Improving the quality of medical images, such as removing noise or enhancing contrast.\n",
        "\n",
        "\n",
        "Q.22 What is mode Collapse inn Gans, and How can it be  prevented?\n",
        "\n",
        "Ans :- Mode Collapse in GANs\n",
        "Mode collapse is a common problem in GAN training where the generator fails to capture the full diversity of the training data and instead focuses on producing a limited set of similar samples. This results in a lack of variety in the generated output.\n",
        "\n",
        "Causes of Mode Collapse:\n",
        "\n",
        "Discriminator Dominance: If the discriminator becomes too powerful, it can easily distinguish between real and fake data, leading the generator to struggle and produce only a few types of samples.\n",
        "Generator's Limited Capacity: The generator's architecture may not be complex enough to capture the full range of variations in the data distribution."
      ],
      "metadata": {
        "id": "Rjl15_Xox__x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Questions"
      ],
      "metadata": {
        "id": "IcQenvoX8PZl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ly5qL16qvf24"
      },
      "outputs": [],
      "source": [
        "# Q.1 Implement a simple GAN archeitecture to generate random images (like noise or basic shapes) using tensorflow/keras?\n",
        "# Ans :-\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Conv2DTranspose, LeakyReLU, Conv2D, Flatten # Import necessary layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# ... (rest of your code remains the same) ..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.2 Implement the discriminator for a GAN with an image input of shape(28,28).\n",
        "# Ans :-\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, LeakyReLU\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "def build_discriminator(image_shape):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=image_shape))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  return model\n",
        "\n",
        "# Example usage:\n",
        "image_shape = (28, 28, 1)  # For grayscale images\n",
        "discriminator = build_discriminator(image_shape)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZxc_QLP9FQB",
        "outputId": "5152b899-1727-4585-d8b8-68792cdccca3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.3 Train the generator to produce simple digits (using noise as input) and plot the generated images.\n",
        "# Ans :-\n",
        "# Q.3 Train the generator to produce simple digits (using noise as input) and plot the generated images.\n",
        "# Ans :-\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Dense, Reshape, Conv2DTranspose, LeakyReLU, Dropout, Flatten, Conv2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np # Import numpy for numerical operations\n",
        "import matplotlib.pyplot as plt # Import matplotlib for plotting\n",
        "\n",
        "# ... (rest of your code remains the same) ..."
      ],
      "metadata": {
        "id": "29ZZUynE9o3G"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.4 Implement WGAN by modifying  the loss function in the GAN.\n",
        "#Ans :-\n",
        "# Q.4 Implement WGAN by modifying  the loss function in the GAN.\n",
        "#Ans :-\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Conv2DTranspose, LeakyReLU, Dropout, Flatten, Conv2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = Adam(learning_rate=0.0002, beta_1=0.5)  # Example: Adam optimizer\n",
        "\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(y_true * y_pred)\n",
        "\n",
        "def gradient_penalty(discriminator, real_images, fake_images):\n",
        "    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
        "    interpolated = alpha * real_images + (1 - alpha) * fake_images\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(interpolated)\n",
        "        pred = discriminator(interpolated)\n",
        "    gradients = tape.gradient(pred, interpolated)\n",
        "    gradients = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
        "    gradient_penalty = tf.reduce_mean((gradients - 1.0) ** 2)\n",
        "    return gradient_penalty\n",
        "\n",
        "# ... (rest of the code for building the generator and discriminator)\n",
        "\n",
        "# Compile the models\n",
        "discriminator.compile(loss=wasserstein_loss, optimizer=optimizer, metrics=['accuracy'])\n",
        "model.compile(loss=wasserstein_loss, optimizer=optimizer)\n",
        "\n",
        "# Train the GAN\n",
        "for epoch in range(epochs):\n",
        "    # Train the discriminator\n",
        "    real_imgs = x_train"
      ],
      "metadata": {
        "id": "wFw71I67-0oP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.5 Use a trained generator to generate  a batch of fake images and display them.\n",
        "# Ans :-\n",
        "# Assuming 'generator' is your trained generator model\n",
        "generator.save('generator_model.h5')\n",
        "generator = load_model('/path/to/your/generator_model.h5')  # Replace with actual path\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # Import numpy for numerical operations\n",
        "\n",
        "# Load the trained generator\n",
        "# Assuming you saved the model as 'generator_model.h5'\n",
        "# If it's in a different directory, specify the full path.\n",
        "generator = load_model('generator_model.h5')\n",
        "\n",
        "# Generate fake images\n",
        "latent_dim = 100  # Adjust based on your generator's input\n",
        "noise = np.random.randn(16, latent_dim)  # Generate random noise\n",
        "generated_images = generator.predict(noise)\n",
        "\n",
        "# Plot the generated images\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(16):\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    plt.imshow(generated_images[i, :, :, 0], cmap='gray')  # Adjust if your images are not grayscale\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "nUj4v_Ln_TQn",
        "outputId": "50e314f9-7f4a-4256-bb6b-228e7f071a42"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = '/path/to/your/generator_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-13a41af3a28c>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Assuming 'generator' is your trained generator model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'generator_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/path/to/your/generator_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace with actual path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    192\u001b[0m         )\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    559\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 561\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/path/to/your/generator_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.6 create a styleGAN-inspired archeitecture that outputs  high-resolution images.\n",
        "# Ans :-\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Conv2DTranspose, LeakyReLU, Add, Concatenate\n",
        "\n",
        "def build_generator(latent_dim, initial_size=4):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Input(shape=(latent_dim,)))\n",
        "    model.add(Dense(initial_size * initial_size * 512))\n",
        "    model.add(Reshape((initial_size, initial_size, 512)))\n",
        "\n",
        "    # Progressive growing stages\n",
        "    for i in range(int(np.log2(image_size)) - 2):\n",
        "        model.add(Conv2DTranspose(256, kernel_size=4, strides=2, padding='same'))\n",
        "        model.add(LeakyReLU())\n",
        "        model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))\n",
        "        model.add(LeakyReLU())\n",
        "        model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))\n",
        "        model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Conv2DTranspose(3, kernel_size=4, strides=2, padding='same', activation='tanh'))\n",
        "\n",
        "    return 1\n",
        "\n",
        "# ... (rest of the code for the discriminator and training loop)"
      ],
      "metadata": {
        "id": "2lubuNPw_3zK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.Implement the Wassertein loss function for GAN training.\n",
        "# Ans :-\n",
        "# Q.Implement the Wassertein loss function for GAN training.\n",
        "# Ans :-\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Conv2DTranspose, LeakyReLU, Dropout, Flatten, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model # Import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# ... (rest of the code for building the generator and discriminator)\n",
        "\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(y_true * y_pred)\n",
        "\n",
        "# Define the optimizer here\n",
        "optimizer = Adam(learning_rate=0.0002, beta_1=0.5)  # Initialize the optimizer\n",
        "\n",
        "# Combine generator and discriminator into a single model for training the generator\n",
        "# This ensures the optimizer is aware of both generator and discriminator weights\n",
        "gan_input = Input(shape=(latent_dim,))\n",
        "generated_image = generator(gan_input)\n",
        "gan_output = discriminator(generated_image)\n",
        "gan = Model(gan_input, gan_output)\n",
        "\n",
        "# Compile the models\n",
        "discriminator.compile(loss=wasserstein_loss, optimizer=optimizer, metrics=['accuracy'])\n",
        "gan.compile(loss=wasserstein_loss, optimizer=optimizer) # Compile the combined model\n",
        "\n",
        "# ... (training loop)\n",
        "\n",
        "# In the training loop, update the discriminator and generator:\n",
        "\n",
        "# Train the discriminator\n",
        "real_imgs = x_train[np.random.randint(0, x_train.shape[0], batch_size)]\n",
        "fake_imgs = generator"
      ],
      "metadata": {
        "id": "qUHWoso8AjnL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.8 Write a function to modify the discriminator to include a dropout layer with a rate of 0.4 and  print the configuration.\n",
        "# Ans :-\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, LeakyReLU, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "def build_discriminator(image_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=image_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.4))  # Add dropout layer\n",
        "    model.add(Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.4))  # Add dropout layer\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "image_shape = (28, 28, 1)  # For grayscale images\n",
        "discriminator = build_discriminator(image_shape)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the model configuration\n",
        "discriminator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "H4cKFf2GBFCB",
        "outputId": "7594dd31-0612-4db1-cbb1-f24ad36ad481"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m640\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_4 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │          \u001b[38;5;34m73,856\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_5 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6272\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │           \u001b[38;5;34m6,273\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6272</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,273</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m80,769\u001b[0m (315.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,769</span> (315.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m80,769\u001b[0m (315.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,769</span> (315.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.9 Write a function to modify the discriminator  to include a dropout layer with rate of  0.4 and print the configuration?\n",
        "# Ans :-\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, LeakyReLU, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "def build_discriminator(image_shape):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=image_shape))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dropout(0.4))  # Add dropout layer\n",
        "  model.add(Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dropout(0.4))  # Add dropout layer\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  return model\n",
        "\n",
        "# Example usage:\n",
        "image_shape = (28, 28, 1)  # For grayscale images\n",
        "discriminator = build_discriminator(image_shape)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "discriminator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "8r-q5KEqBpPE",
        "outputId": "04cef9d8-2e79-4dec-f114-53f1a22316eb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m640\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_6 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │          \u001b[38;5;34m73,856\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_7 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6272\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │           \u001b[38;5;34m6,273\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6272</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,273</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m80,769\u001b[0m (315.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,769</span> (315.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m80,769\u001b[0m (315.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,769</span> (315.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "viwAr_dDCTv3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}